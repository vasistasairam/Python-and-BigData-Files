{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we first load the glove vectors as a dictionary - `embeddings_index`\n",
    "`embeddings_index['banana']` would give some 100 length vector for the word `'banana'`\n",
    "\n",
    "The object `GLOVE_DIR` points to the text file which containes the vectors, but it could also be downloaded form http://nlp.stanford.edu/data/glove.6B.zip and saved on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2776: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f427da28b050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGLOVE_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'glove.6B.100d.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2776: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "GLOVE_DIR = 'C://Piazza//Activity Lab//DeepLearning//glove.6B'\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "type(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34028  ,  0.46436  , -0.083324 ,  0.20186  , -0.17831  ,\n",
       "       -0.4663   ,  0.61793  ,  0.30129  ,  0.5728   , -0.34783  ,\n",
       "       -0.9216   ,  0.30484  ,  0.30382  ,  0.58035  ,  0.12112  ,\n",
       "        0.77288  ,  1.1547   , -0.576    ,  0.51471  ,  0.21552  ,\n",
       "        0.21106  ,  0.67875  ,  1.1962   ,  0.11142  ,  0.50809  ,\n",
       "        1.1873   ,  0.035288 , -0.88952  ,  0.042803 , -0.36714  ,\n",
       "        0.37993  ,  0.61945  ,  1.0194   , -0.95084  , -0.0072258,\n",
       "        0.69454  ,  0.38692  , -0.18544  ,  0.2885   , -0.81279  ,\n",
       "       -0.46473  , -0.82623  ,  0.42778  , -0.14064  ,  0.30173  ,\n",
       "        0.074418 , -0.40044  ,  0.33969  , -0.62917  , -0.054449 ,\n",
       "       -0.78469  ,  0.2354   , -0.78359  ,  0.74708  , -0.31074  ,\n",
       "       -0.07038  , -0.34623  ,  0.33849  ,  0.89621  ,  0.30288  ,\n",
       "        0.012978 ,  0.020869 , -0.14436  , -0.40914  ,  0.16651  ,\n",
       "       -0.88124  , -0.078419 ,  0.048156 ,  0.27032  , -0.81761  ,\n",
       "        0.027778 ,  0.62487  ,  0.1549   , -0.15838  ,  0.088675 ,\n",
       "        0.063411 , -0.14473  , -0.0066816, -0.18535  ,  1.5642   ,\n",
       "        0.3726   , -0.81706  , -0.021685 ,  0.91209  , -0.35784  ,\n",
       "       -0.98389  , -0.37103  , -0.10909  ,  0.18898  , -0.33884  ,\n",
       "       -0.058326 ,  0.41438  , -1.0411   , -0.42643  , -0.50664  ,\n",
       "       -0.75863  , -0.15815  , -0.1831   ,  0.7343   , -0.26852  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['banana']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find the top 7 words that are closest to 'compute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "u = embeddings_index['compute']\n",
    "norm_u = np.linalg.norm(u)\n",
    "similarity = []\n",
    "\n",
    "for word in embeddings_index.keys():\n",
    "    v = embeddings_index[word]\n",
    "    cosine = np.dot(u, v)/norm_u/np.linalg.norm(v)\n",
    "    similarity.append((word, cosine))\n",
    "\n",
    "print(len(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compute', 1.0),\n",
       " ('calculate', 0.7222062),\n",
       " ('algorithm', 0.6441058),\n",
       " ('computed', 0.61362344),\n",
       " ('algorithms', 0.6134383),\n",
       " ('equivalently', 0.5999141),\n",
       " ('formula_1', 0.5970425),\n",
       " ('formula_2', 0.5948517),\n",
       " ('formula_3', 0.593129),\n",
       " ('formula_5', 0.5920933)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(similarity, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do vector algebra.\n",
    "\n",
    "### First we subtract the vector for `italy` from `paris`. This could be imagined as a vector pointing from country to its capital. Then we add the vector of `nepal`. Let's see if it does point to the country's capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "output = embeddings_index['paris'] - embeddings_index['italy'] + embeddings_index['nepal']\n",
    "norm_out = np.linalg.norm(u)\n",
    "\n",
    "similarity = []\n",
    "for word in embeddings_index.keys():\n",
    "    v = embeddings_index[word]\n",
    "    cosine = np.dot(output, v)/norm_out/np.linalg.norm(v)\n",
    "    similarity.append((word, cosine))\n",
    "\n",
    "print(len(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kathmandu', 1.1744573),\n",
       " ('nepal', 1.0729638),\n",
       " ('katmandu', 1.0361387),\n",
       " ('kampala', 1.0096042),\n",
       " ('bangkok', 0.97564006),\n",
       " ('dhaka', 0.937862),\n",
       " ('nairobi', 0.9219297)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(similarity, key=lambda x: x[1], reverse=True)[:7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
