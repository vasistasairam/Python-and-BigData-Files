{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**Apache Spark** is an unfied computing engine and a set of libraries for parallel data processing on computer clusters.\n",
    "<br>-**Spark** is the most actively developed open source engine for this task, making it a standard tool for any developer or data scientist interested in big data.\n",
    "<br>-**Spark** supports multiple widely used programming languages (Python, Java, Scala, and R).\n",
    "<br>-Includes libraries for diverse tasks ranging from SQL to streaming and machine learning, and runs anywhere from a laptop to a cluster of thousands of servers.\n",
    "\n",
    "<br>**Spark's toolkit - all the components and libraries Spark offers to end-users.**\n",
    "\n",
    "![](./Images/RDD_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.6-src.zip\") # Path name to the directory where the spark2 client is installed\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\") # path name to the directory where the pyspark libraries are located "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Applications\n",
    "-  Spark applications consists of a _driver_ process and a set of _executor_ processes.\n",
    "-  The driver process runs the main() function, sits on a node in the cluster and is responsible for\n",
    "    -  maintaining information about spark application.\n",
    "    -  responding to a user's program or input\n",
    "    -  analyzing, distributing and scheduling the work across the executors\n",
    "    \n",
    "<br>**Architecture of a Spark Application**\n",
    "![](./Images/RDD_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SparkSession\n",
    "-  Spark Application is controlled through a driver process called the SparkSession.\n",
    "-  The SparkSession instance is the way Spark executes user-defined manipulations across the cluster. \n",
    "-  There is a one-to-one correspondence between a SparkSession and a Spark Application.\n",
    "-  In Scala and Python, the variable is available as spark when you start the console.\n",
    "\n",
    "-  When you start Spark in the interactive mode, you implicitly create a SparkSession that manages the Spark Application. \n",
    "\n",
    "-  When you start it through a standalone application, you must create the SparkSession object yourself in your application code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Spark RDD Application\") \\\n",
    "        .config(conf = SparkConf()) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://r.insofe.edu.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark RDD Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f42bc0d4d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://r.insofe.edu.in:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark RDD Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark RDD Application>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resiliebt Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "-  There are two sets of low-level APIs: \n",
    "    -  For manipulating distributed data (RDDs)\n",
    "    -  For distributing and manipulating distributed shared variables \n",
    "        -  broadcast variables\n",
    "        -  accumulators\n",
    "        \n",
    "-  All Spark workloads compile down to these fundamental primitives. \n",
    "-  A SparkContext is the entry point for low-level API functionality. \n",
    "-  SparkContext can be accessed through the SparkSession, which is the tool you use to perform computation across a Spark cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "-  Spark introduces the concept of an RDD (Resilient Distributed Dataset), an immutable fault-tolerant, distributed collection of objects that can be operated on in parallel.\n",
    "-  An RDD can contain any type of object and is created by loading an external dataset or distributing a collection from the driver program.\n",
    "\n",
    "\n",
    "** There are three ways to create an RDD in Spark.**\n",
    "-  Parallelizing already existing collection in driver program.\n",
    "-  Referencing a dataset in an external storage system (e.g. HDFS, Hbase, shared file system).\n",
    "-  Creating RDD from already existing RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_range = range(1,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_RDD = sc.parallelize(data_range,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(data_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pair = [(\"maths\",52),(\"english\",75),(\"science\",82), (\"computer\",65),(\"maths\",85)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(data_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile(\"file:///home/jayantm/Batches/Batch48/SparkRDD/temp_data.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1901\\t-78\\t1',\n",
       " '1901\\t-72\\t1',\n",
       " '1901\\t-94\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-83\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-83\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-161\\t1',\n",
       " '1901\\t-128\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-111\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-111\\t1',\n",
       " '1901\\t-106\\t1',\n",
       " '1901\\t-133\\t1',\n",
       " '1901\\t-106\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-183\\t1',\n",
       " '1901\\t-150\\t1',\n",
       " '1901\\t-128\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-128\\t1',\n",
       " '1901\\t-128\\t1',\n",
       " '1901\\t-128\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-100\\t1',\n",
       " '1901\\t-183\\t1',\n",
       " '1901\\t-178\\t1',\n",
       " '1901\\t-211\\t1',\n",
       " '1901\\t-250\\t1',\n",
       " '1901\\t-200\\t1',\n",
       " '1901\\t-178\\t1',\n",
       " '1901\\t-106\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-133\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-94\\t1',\n",
       " '1901\\t-94\\t1',\n",
       " '1901\\t-89\\t1',\n",
       " '1901\\t-144\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-83\\t1',\n",
       " '1901\\t-89\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-72\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-89\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-139\\t1',\n",
       " '1901\\t-172\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-144\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-172\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-167\\t1',\n",
       " '1901\\t-183\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-106\\t1',\n",
       " '1901\\t-150\\t1',\n",
       " '1901\\t-111\\t1',\n",
       " '1901\\t-111\\t1',\n",
       " '1901\\t-106\\t1',\n",
       " '1901\\t-72\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-178\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-167\\t1',\n",
       " '1901\\t-194\\t1',\n",
       " '1901\\t-150\\t1',\n",
       " '1901\\t-172\\t1',\n",
       " '1901\\t-200\\t1',\n",
       " '1901\\t-133\\t1',\n",
       " '1901\\t-167\\t1',\n",
       " '1901\\t-189\\t1',\n",
       " '1901\\t-106\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-100\\t1',\n",
       " '1901\\t-100\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-139\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-122\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-83\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-111\\t1',\n",
       " '1901\\t-133\\t1',\n",
       " '1901\\t-178\\t1',\n",
       " '1901\\t-94\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-200\\t1',\n",
       " '1901\\t-100\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-183\\t1',\n",
       " '1901\\t-89\\t1',\n",
       " '1901\\t-172\\t1',\n",
       " '1901\\t-172\\t1',\n",
       " '1901\\t-72\\t1',\n",
       " '1901\\t-89\\t1',\n",
       " '1901\\t-117\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-111\\t1',\n",
       " '1901\\t-156\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t11\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t139\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t139\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t189\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t211\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t206\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t239\\t1',\n",
       " '1901\\t222\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t222\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t222\\t1',\n",
       " '1901\\t206\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t211\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t206\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t228\\t1',\n",
       " '1901\\t206\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t217\\t1',\n",
       " '1901\\t206\\t1',\n",
       " '1901\\t189\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t211\\t1',\n",
       " '1901\\t189\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t189\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t200\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t217\\t1',\n",
       " '1901\\t194\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t217\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t178\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t189\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t139\\t1',\n",
       " '1901\\t139\\t1',\n",
       " '1901\\t139\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t172\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t139\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t150\\t1',\n",
       " '1901\\t144\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t167\\t1',\n",
       " '1901\\t183\\t1',\n",
       " '1901\\t156\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t161\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t133\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t122\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t117\\t1',\n",
       " '1901\\t128\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t106\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t111\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t100\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t94\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t44\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t89\\t1',\n",
       " '1901\\t83\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t72\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t78\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t67\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t39\\t1',\n",
       " '1901\\t61\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t56\\t1',\n",
       " '1901\\t50\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t17\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-67\\t1',\n",
       " '1901\\t-78\\t1',\n",
       " '1901\\t-72\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t6\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-39\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-50\\t1',\n",
       " '1901\\t-61\\t1',\n",
       " '1901\\t-56\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t0\\t1',\n",
       " '1901\\t-6\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t-17\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t28\\t1',\n",
       " '1901\\t33\\t1',\n",
       " '1901\\t22\\t1',\n",
       " '1901\\t-11\\t1',\n",
       " '1901\\t-22\\t1',\n",
       " '1901\\t-28\\t1',\n",
       " '1901\\t-33\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-44\\t1',\n",
       " '1901\\t-89\\t1',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda s : s.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd3.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs support two types of operations:\n",
    "* Transformations are operations (such as map, filter, join, union, and so on) that are performed on an RDD and which yield a new RDD containing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actions are operations (such as reduce, count, first, and so on) that return a value after running a computation on an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transformations in Spark are “lazy”, meaning that they do not compute their results right away. \n",
    "* They just “remember” the operation to be performed and the dataset (e.g., file) to which the operation is to be    performed. \n",
    "* The transformations are only actually computed when an action is called and the result is returned to the driver program. \n",
    "* This design enables Spark to run more efficiently. For example, if a big file was transformed in various ways and passed to first action, Spark would only process and return the result for the first line, rather than do the work for the entire file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* coalesce() - Return a new RDD that is reduced into numPartitions partitions.\n",
    "* glom() - Return an RDD created by coalescing all elements within each partition into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD = sc.parallelize(range(30), 5)\n",
    "RDD.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD = RDD.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RDD.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD_par = RDD.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD_par.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Note_ :  _Internally, repartition uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Map Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intRdd = sc.parallelize([10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapRDD = intRdd.map(lambda x : x**2)\n",
    "mapRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter(Transformation):\n",
    "    \n",
    "* The filter operation evaluates a Boolean function for each data item of the RDD\n",
    " and puts the items for which the function returned true into the resulting RDD. Filter\n",
    " is a Transformation. Collect is an Action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numRdd = sc.parallelize([11,12,13,14,15,16,17,18])\n",
    "filterRdd1 = numRdd.filter(lambda x : x%2 == 1)\n",
    "filterRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filterRdd2 = numRdd.filter(lambda x : x%2 == 0)\n",
    "filterRdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ReduceByKey (Transformation):\n",
    "* Spark RDD reduceByKey function merges the values for each key using an associative reduce function. Basically reduceByKey function works only for RDDs which contains key and value pairs kind of elements (i.e. RDDs having tuple or Map as a data element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"comp\", 2), (\"tab\", 1), (\"comp\", 1), (\"comp\", 1),\n",
    "(\"tab\", 1), (\"tab\", 1), (\"tab\", 1), (\"tab\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* flatMap (Transformation) :\n",
    "* Spark flatMap function returns a new RDD by first applying a function to all elements of this RDD, and then flattening the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentRdd = sc.parallelize([\"Hello Participants at IBM\", \"Welcome to Big Data Training.\", \"We are doing pySpark Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentRdd.map(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = sentRdd.flatMap(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* groupByKey(Transformation):\n",
    "* Spark groupByKey function returns a new RDD. The returned RDD gives back an object which allows to iterate over the results. The results of groupByKey returns a list by calling list() on values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = sc.parallelize([('x',1), ('x',1), ('y', 1), ('z', 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itRdd = example.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itRdd.map(lambda x :(x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* groupBy (Transformation) :\n",
    "* groupBy function returns an RDD of grouped items. This operation will return the new RDD which basically is made up with a KEY (which is a group) and list of items of that group (in a form of Iterator). Order of element within the group may not same when you apply the same operation on the same RDD over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namesRdd = sc.parallelize([\"Joseph\", \"Jimmy\", \"Tina\",\"Thomas\",\"James\",\"Cory\",\"Christine\", \"Jackeline\", \"Juan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namesRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result =namesRdd.groupBy(lambda word: word[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(x, sorted(y)) for (x, y) in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mapValues (Transformation) :\n",
    "* Apply a function to each value of a pair RDD without changing the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namesRdd = sc.parallelize([\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\",\"eagle\"])\n",
    "pairRdd = namesRdd.map(lambda x :(len(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pairRdd.mapValues(lambda y: \"Animal name is \" + y)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join (pair Rdd Transformation): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"red\",20),(\"red\",30),(\"blue\", 100)])\n",
    "rdd2 = sc.parallelize([(\"red\",40),(\"red\",50),(\"yellow\", 10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* inner join and outer join (Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"Mercedes\", \"E-Class\"), (\"Toyota\", \"Corolla\"),(\"Renault\", \"Duster\")])\n",
    "rdd2 = sc.parallelize([(\"Mercedes\", \"C-Class\"), (\"Toyota\", \"Prius\"),(\"Toyota\", \"Etios\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innerJoinRdd = rdd1.join(rdd2)\n",
    "innerJoinRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outerJoinRdd = rdd1.leftOuterJoin(rdd2)\n",
    "outerJoinRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Union:\n",
    "* Combines the values in various Rdds to form a cohesive unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1= [('k1', 1), ('k2', 2), ('k3', 5)]\n",
    "d2= [('k1', 3), ('k2',4), ('k4', 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1_RDD = sc.parallelize(d1)\n",
    "d2_RDD = sc.parallelize(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3_union = d1_RDD.union(d2_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3_union.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* collect (Action):\n",
    "* Collect action returns the results or the value. When an action is called transformations are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jayantm/Batches/Batch48/SparkRDD\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\r\n",
      "Overview\r\n",
      "Programming Guides\r\n",
      "API Docs\r\n",
      "Deploying\r\n",
      "More\r\n",
      "Spark Overview\r\n",
      "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\r\n",
      "\r\n",
      "Downloading\r\n"
     ]
    }
   ],
   "source": [
    "!head input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile('file:///home/jayantm/Batches/Batch48/SparkRDD/input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.3.0',\n",
       " 'Overview',\n",
       " 'Programming Guides',\n",
       " 'API Docs',\n",
       " 'Deploying',\n",
       " 'More',\n",
       " 'Spark Overview',\n",
       " 'Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.',\n",
       " '',\n",
       " 'Downloading',\n",
       " 'Get Spark from the downloads page of the project website. This documentation is for Spark version 2.3.0. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and in the future Python users can also install Spark from PyPI.',\n",
       " '',\n",
       " 'If you’d like to build Spark from source, visit Building Spark.',\n",
       " '',\n",
       " 'Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS). It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.',\n",
       " '',\n",
       " 'Spark runs on Java 8+, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.3.0 uses Scala 2.11. You will need to use a compatible Scala version (2.11.x).',\n",
       " '',\n",
       " 'Note that support for Java 7, Python 2.6 and old Hadoop versions before 2.6.5 were removed as of Spark 2.2.0. Support for Scala 2.10 was removed as of 2.3.0.',\n",
       " '',\n",
       " 'Running the Examples and Shell',\n",
       " 'Spark comes with several sample programs. Scala, Java, Python and R examples are in the examples/src/main directory. To run one of the Java or Scala sample programs, use bin/run-example <class> [params] in the top-level Spark directory. (Behind the scenes, this invokes the more general spark-submit script for launching applications). For example,',\n",
       " '',\n",
       " './bin/run-example SparkPi 10',\n",
       " 'You can also run Spark interactively through a modified version of the Scala shell. This is a great way to learn the framework.',\n",
       " '',\n",
       " './bin/spark-shell --master local[2]',\n",
       " 'The --master option specifies the master URL for a distributed cluster, or local to run locally with one thread, or local[N] to run locally with N threads. You should start by using local for testing. For a full list of options, run Spark shell with the --help option.',\n",
       " '',\n",
       " 'Spark also provides a Python API. To run Spark interactively in a Python interpreter, use bin/pyspark:',\n",
       " '',\n",
       " './bin/pyspark --master local[2]',\n",
       " 'Example applications are also provided in Python. For example,',\n",
       " '',\n",
       " './bin/spark-submit examples/src/main/python/pi.py 10',\n",
       " 'Spark also provides an experimental R API since 1.4 (only DataFrames APIs included). To run Spark interactively in a R interpreter, use bin/sparkR:',\n",
       " '',\n",
       " './bin/sparkR --master local[2]',\n",
       " 'Example applications are also provided in R. For example,',\n",
       " '',\n",
       " './bin/spark-submit examples/src/main/r/dataframe.R',\n",
       " 'Launching on a Cluster',\n",
       " 'The Spark cluster mode overview explains the key concepts in running on a cluster. Spark can run both by itself, or over several existing cluster managers. It currently provides several options for deployment:',\n",
       " '',\n",
       " 'Standalone Deploy Mode: simplest way to deploy Spark on a private cluster',\n",
       " 'Apache Mesos',\n",
       " 'Hadoop YARN',\n",
       " 'Kubernetes',\n",
       " 'Where to Go from Here',\n",
       " 'Programming Guides:',\n",
       " '',\n",
       " 'Quick Start: a quick introduction to the Spark API; start here!',\n",
       " 'RDD Programming Guide: overview of Spark basics - RDDs (core but old API), accumulators, and broadcast variables',\n",
       " 'Spark SQL, Datasets, and DataFrames: processing structured data with relational queries (newer API than RDDs)',\n",
       " 'Structured Streaming: processing structured data streams with relation queries (using Datasets and DataFrames, newer API than DStreams)',\n",
       " 'Spark Streaming: processing data streams using DStreams (old API)',\n",
       " 'MLlib: applying machine learning algorithms',\n",
       " 'GraphX: processing graphs',\n",
       " 'API Docs:',\n",
       " '',\n",
       " 'Spark Scala API (Scaladoc)',\n",
       " 'Spark Java API (Javadoc)',\n",
       " 'Spark Python API (Sphinx)',\n",
       " 'Spark R API (Roxygen2)',\n",
       " 'Spark SQL, Built-in Functions (MkDocs)',\n",
       " 'Deployment Guides:',\n",
       " '',\n",
       " 'Cluster Overview: overview of concepts and components when running on a cluster',\n",
       " 'Submitting Applications: packaging and deploying applications',\n",
       " 'Deployment modes:',\n",
       " 'Amazon EC2: scripts that let you launch a cluster on EC2 in about 5 minutes',\n",
       " 'Standalone Deploy Mode: launch a standalone cluster quickly without a third-party cluster manager',\n",
       " 'Mesos: deploy a private cluster using Apache Mesos',\n",
       " 'YARN: deploy Spark on top of Hadoop NextGen (YARN)',\n",
       " 'Kubernetes: deploy Spark on top of Kubernetes',\n",
       " 'Other Documents:',\n",
       " '',\n",
       " 'Configuration: customize Spark via its configuration system',\n",
       " 'Monitoring: track the behavior of your applications',\n",
       " 'Tuning Guide: best practices to optimize performance and memory use',\n",
       " 'Job Scheduling: scheduling resources across and within Spark applications',\n",
       " 'Security: Spark security support',\n",
       " 'Hardware Provisioning: recommendations for cluster hardware',\n",
       " 'Integration with other storage systems:',\n",
       " 'Cloud Infrastructures',\n",
       " 'OpenStack Swift',\n",
       " 'Building Spark: build Spark using the Maven system',\n",
       " 'Contributing to Spark',\n",
       " 'Third Party Projects: related third party Spark projects',\n",
       " 'External Resources:',\n",
       " '',\n",
       " 'Spark Homepage',\n",
       " 'Spark Community resources, including local meetups',\n",
       " 'StackOverflow tag apache-spark',\n",
       " 'Mailing Lists: ask questions about Spark here',\n",
       " 'AMP Camps: a series of training camps at UC Berkeley that featured talks and exercises about Spark, Spark Streaming, Mesos, and more. Videos, slides and exercises are available online for free.',\n",
       " 'Code Examples: more are also available in the examples subfolder of Spark (Scala, Java, Python, R)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* takeOrdered(Action):\n",
    "* Orders the data items of the RDD using their inherent implicit ordering function and returns the first n items as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([\"dog\", \"cat\", \"ape\", \"salmon\", \"gnu\"])\n",
    "rdd1.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduce (Action):\n",
    "* This function provides the well-known reduce functionality in Spark. Please note that any function f you provide, should be commutative in order to generate reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intVals = range(1,15)\n",
    "numRdd = sc.parallelize(intVals)\n",
    "cSum = numRdd.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"file:///home/jayantm/Batches/Batch48/SparkRDD/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts.saveAsTextFile(\"hdfs:///user/insofe/jayantm/SparkRDDOut/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/insofe/jayantm/SparkRDDOut/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed shared variables\n",
    "* Broadcast variable\n",
    "* Accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_collection = \"Postgraduate Program in Big Data Analytics and Optimization\"\\\n",
    "  .split(\" \")\n",
    "    \n",
    "words = sc.parallelize(my_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supplementalData = {\"Postgraduate\":1000, \"Analytics\":200, \"Optimization\": 400,\n",
    "                    \"Big\":-300, \"Data\": 100, \"Program\":100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suppBroadcast = sc.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.map(lambda word: (word,suppBroadcast.value.get(word))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\\\n",
    "  .sortBy(lambda wordPair: wordPair[1])\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num.foreach(lambda x: count.add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
