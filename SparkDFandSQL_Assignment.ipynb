{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create SparkContext, SparkSession\n",
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = 'hdfs:///apps/hive/warehouse/'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://w.insofe.edu.in:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL Hive integration example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x3a09510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Example Data - Departments and Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Deep Learning')\n",
      "Row(firstName='charlie', lastName='delta', email='no-reply@insofe.edu.con', salary=120000)\n",
      "no-reply@insofe.edu.in\n"
     ]
    }
   ],
   "source": [
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Deep Learning')\n",
    "department2 = Row(id='789012', name='IoT')\n",
    "department3 = Row(id='345678', name='Consulting')\n",
    "department4 = Row(id='901234', name='Block Chain')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('echo', 'france', 'no-reply@insofe.edu.in', 100000)\n",
    "employee2 = Employee('charlie', 'delta', 'no-reply@insofe.edu.con', 120000)\n",
    "employee3 = Employee('alpha', None, 'no-reply@insofe.edu.net', 140000)\n",
    "employee4 = Employee(None, 'beta', 'no-reply@insofe.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print department1\n",
    "print employee2\n",
    "print departmentWithEmployees1.employees[0].email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrames from a list of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "|department             |employees                                                                                          |\n",
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "|[123456, Deep Learning]|[[echo, france, no-reply@insofe.edu.in, 100000], [charlie, delta, no-reply@insofe.edu.con, 120000]]|\n",
      "|[789012, IoT]          |[[alpha,, no-reply@insofe.edu.net, 140000], [, beta, no-reply@insofe.edu, 160000]]                 |\n",
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "\n",
    "# Create a DataFrame for the above list - departmentsWithEmployeesSeq1\n",
    "df1 = \n",
    "\n",
    "# Display the DataFrame contents df1\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------------------------------------------------------------------------+\n",
      "|department           |employees                                                                                     |\n",
      "+---------------------+----------------------------------------------------------------------------------------------+\n",
      "|[345678, Consulting] |[[echo, france, no-reply@insofe.edu.in, 100000], [, beta, no-reply@insofe.edu, 160000]]       |\n",
      "|[901234, Block Chain]|[[charlie, delta, no-reply@insofe.edu.con, 120000], [alpha,, no-reply@insofe.edu.net, 140000]]|\n",
      "+---------------------+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "\n",
    "# Create a DataFrame for the above list - departmentsWithEmployeesSeq2\n",
    "df2 = \n",
    "\n",
    "# Display the DataFrame contents df2\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "|department             |employees                                                                                          |\n",
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "|[123456, Deep Learning]|[[echo, france, no-reply@insofe.edu.in, 100000], [charlie, delta, no-reply@insofe.edu.con, 120000]]|\n",
      "|[789012, IoT]          |[[alpha,, no-reply@insofe.edu.net, 140000], [, beta, no-reply@insofe.edu, 160000]]                 |\n",
      "|[345678, Consulting]   |[[echo, france, no-reply@insofe.edu.in, 100000], [, beta, no-reply@insofe.edu, 160000]]            |\n",
      "|[901234, Block Chain]  |[[charlie, delta, no-reply@insofe.edu.con, 120000], [alpha,, no-reply@insofe.edu.net, 140000]]     |\n",
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union the above two DataFrames df1 and df2 (using union all)\n",
    "unionDF = \n",
    "\n",
    "# Display the DataFrame contents unionDF\n",
    "unionDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the unioned DataFrame to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the file(directory) if it exists\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "## delete only if file exists ##\n",
    "filepath = \"/home/<user-id>/tmp/spark-df-example.parquet/\"\n",
    "filename = \"file:///home/<user-id>/tmp/spark-df-example.parquet/\"\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    shutil.rmtree(filepath)\n",
    "\n",
    "# Write unionDF to a Parquet file\n",
    "unionDF.write.parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a DataFrame from the Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "|department             |employees                                                                                          |\n",
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "|[123456, Deep Learning]|[[echo, france, no-reply@insofe.edu.in, 100000], [charlie, delta, no-reply@insofe.edu.con, 120000]]|\n",
      "|[901234, Block Chain]  |[[charlie, delta, no-reply@insofe.edu.con, 120000], [alpha,, no-reply@insofe.edu.net, 140000]]     |\n",
      "|[345678, Consulting]   |[[echo, france, no-reply@insofe.edu.in, 100000], [, beta, no-reply@insofe.edu, 160000]]            |\n",
      "|[789012, IoT]          |[[alpha,, no-reply@insofe.edu.net, 140000], [, beta, no-reply@insofe.edu, 160000]]                 |\n",
      "+-----------------------+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF = \n",
    "parquetDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode the employees column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------------------+------+\n",
      "|firstName|lastName|email                  |salary|\n",
      "+---------+--------+-----------------------+------+\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|alpha    |null    |no-reply@insofe.edu.net|140000|\n",
      "|null     |beta    |no-reply@insofe.edu    |160000|\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "|null     |beta    |no-reply@insofe.edu    |160000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|alpha    |null    |no-reply@insofe.edu.net|140000|\n",
      "+---------+--------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# As the employees column contains many details about each employee - explode employee data into a seperate columns\n",
    "# using the function explode\n",
    "df = unionDF.select(explode(\"employees\").alias(\"e\"))\n",
    "\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "explodeDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use filter() to return the rows that match a predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------------------+------+\n",
      "|firstName|lastName|email                  |salary|\n",
      "+---------+--------+-----------------------+------+\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "+---------+--------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter rows whose first name is charlie and display in the ascending order of lastname\n",
    "filterDF = explodeDF.\n",
    "filterDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------------------+------+\n",
      "|firstName|lastName|email                  |salary|\n",
      "+---------+--------+-----------------------+------+\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "+---------+--------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# filter rows whose first name is charlie or echo and display in the ascending order of lastname\n",
    "# Use '|' instead of 'or'\n",
    "filterDF = explodeDF.\n",
    "filterDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The where() clause is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------------------+------+\n",
      "|firstName|lastName|email                  |salary|\n",
      "+---------+--------+-----------------------+------+\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "+---------+--------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter rows whose first name is charlie or echo and display in the ascending order of lastname - Using where clause\n",
    "# Use '|' instead of 'or'\n",
    "whereDF = explodeDF.\n",
    "whereDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace null values with -- using DataFrame Na function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------------------+------+\n",
      "|firstName|lastName|email                  |salary|\n",
      "+---------+--------+-----------------------+------+\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|alpha    |--      |no-reply@insofe.edu.net|140000|\n",
      "|--       |beta    |no-reply@insofe.edu    |160000|\n",
      "|echo     |france  |no-reply@insofe.edu.in |100000|\n",
      "|--       |beta    |no-reply@insofe.edu    |160000|\n",
      "|charlie  |delta   |no-reply@insofe.edu.con|120000|\n",
      "|alpha    |--      |no-reply@insofe.edu.net|140000|\n",
      "+---------+--------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill null values with \"--\"\n",
    "\n",
    "nonNullDF = explodeDF.\n",
    "nonNullDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve only rows with missing firstName or lastName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------------------+------+\n",
      "|firstName|lastName|email                  |salary|\n",
      "+---------+--------+-----------------------+------+\n",
      "|null     |beta    |no-reply@insofe.edu    |160000|\n",
      "|null     |beta    |no-reply@insofe.edu    |160000|\n",
      "|alpha    |null    |no-reply@insofe.edu.net|140000|\n",
      "|alpha    |null    |no-reply@insofe.edu.net|140000|\n",
      "+---------+--------+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrive rows where first and last names are nulls\n",
    "\n",
    "filterNonNullDF = explodeDF.\n",
    "filterNonNullDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example aggregations using agg() and countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------+\n",
      "|firstName|lastName|count(DISTINCT firstName)|\n",
      "+---------+--------+-------------------------+\n",
      "|charlie  |delta   |1                        |\n",
      "|alpha    |null    |1                        |\n",
      "|echo     |france  |1                        |\n",
      "|null     |beta    |0                        |\n",
      "+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Count the persons with the distinct firstname and grouping by first name and last name (use countDistinct)\n",
    "\n",
    "countDistinctDF = explodeDF.\n",
    "\n",
    "\n",
    "countDistinctDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the DataFrame and SQL query physical plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: They should be the same **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[firstName#61, lastName#62], functions=[count(distinct firstName#61)])\n",
      "+- *(4) HashAggregate(keys=[firstName#61, lastName#62], functions=[partial_count(distinct firstName#61)])\n",
      "   +- *(4) HashAggregate(keys=[firstName#61, lastName#62, firstName#61], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#61, lastName#62, firstName#61, 200)\n",
      "         +- *(3) HashAggregate(keys=[firstName#61, lastName#62, firstName#61], functions=[])\n",
      "            +- *(3) Project [e#59.firstName AS firstName#61, e#59.lastName AS lastName#62]\n",
      "               +- Generate explode(employees#1), false, [e#59]\n",
      "                  +- Union\n",
      "                     :- *(1) Project [employees#1]\n",
      "                     :  +- Scan ExistingRDD[department#0,employees#1]\n",
      "                     +- *(2) Project [employees#13]\n",
      "                        +- Scan ExistingRDD[department#12,employees#13]\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[firstName#61, lastName#62], functions=[count(distinct firstName#61)])\n",
      "+- *(4) HashAggregate(keys=[firstName#61, lastName#62], functions=[partial_count(distinct firstName#61)])\n",
      "   +- *(4) HashAggregate(keys=[firstName#61, lastName#62, firstName#61], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#61, lastName#62, firstName#61, 200)\n",
      "         +- *(3) HashAggregate(keys=[firstName#61, lastName#62, firstName#61], functions=[])\n",
      "            +- *(3) Project [e#59.firstName AS firstName#61, e#59.lastName AS lastName#62]\n",
      "               +- Generate explode(employees#1), false, [e#59]\n",
      "                  +- Union\n",
      "                     :- *(1) Project [employees#1]\n",
      "                     :  +- Scan ExistingRDD[department#0,employees#1]\n",
      "                     +- *(2) Project [employees#13]\n",
      "                        +- Scan ExistingRDD[department#12,employees#13]\n"
     ]
    }
   ],
   "source": [
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodeDF.registerTempTable(\"sql_table_df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return 'explain'\n",
    "countDistinctDF_sql = spark.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM sql_table_df_example GROUP BY firstName, lastName\")\n",
    "\n",
    "countDistinctDF_sql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum up all the salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|    1040000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salarySumDF = explodeDF.\n",
    "salarySumDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(explodeDF.salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the summary statistics for the salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|          130000.0|\n",
      "| stddev|23904.572186687874|\n",
      "|    min|            100000|\n",
      "|    max|            160000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodeDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example using pandas and Matplotlib integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5a35210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5bc0ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "pdDF = nonNullDF.toPandas()\n",
    "pdDF.plot(x='firstName', y='salary', kind='bar', rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the file df_sample\n",
    "Copy df_sample and df_sample_2.csv files to /home/user-id/tmp/ location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id|end_date|start_date|location\r\n",
      "1|2018-10-10 00:00:00|2018-09-14 00:00:00|TS-HYD\r\n",
      "2|2018-10-11 01:00:20|2018-08-14 00:00:00|KA-BLR\r\n",
      "3|2018-10-12 02:30:00|2018-01-14 00:00:00|TN-CHE\r\n",
      "4|2018-10-13 03:00:20|2018-02-14 00:00:00|MH-MUM\r\n",
      "5|2018-10-14 04:30:00|2018-04-14 00:00:00|TS-HYD"
     ]
    }
   ],
   "source": [
    "!cat /home/<user-id>/tmp/df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+\n",
      "| id|           end_date|         start_date|location|\n",
      "+---+-------------------+-------------------+--------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|\n",
      "+---+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of registering a UDF, call the builtin functions to perform operations on the columns.\n",
    "#### This will provide a performance improvement as the builtins compile and run in the platform's JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to a Date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'date' as the date column from 'end_date'\n",
    "df = df.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|\n",
      "+---+-------------------+-------------------+--------+----------+----------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|2018-10-12|2018-10-12|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|2018-10-13|2018-10-13|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|2018-10-14|2018-10-14|\n",
      "+---+-------------------+-------------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse out the date only\n",
    "# Create a new column 'date_only' from 'end_date' and extract only the date part\n",
    "df = df.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a string and index a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10| HYD|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11| BLR|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|2018-10-12|2018-10-12| CHE|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|2018-10-13|2018-10-13| MUM|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|2018-10-14|2018-10-14| HYD|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the 'location' column using the appropriate seperator and extract city name into new column 'city'\n",
    "df = df.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a date diff function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10| HYD|       26|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11| BLR|       58|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|2018-10-12|2018-10-12| CHE|      271|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|2018-10-13|2018-10-13| MUM|      241|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|2018-10-14|2018-10-14| HYD|      183|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'date_diff' with difference of end_date and start_date in number of days\n",
    "df = df.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10| HYD|       26|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11| BLR|       58|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|2018-10-12|2018-10-12| CHE|      271|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|2018-10-13|2018-10-13| MUM|      241|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|2018-10-14|2018-10-14| HYD|      183|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"sample_df_tbl\")\n",
    "\n",
    "# Display the contents of the above SQL table\n",
    "spark.sql(\"select * from sample_df_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the DataFrame back to JSON strings.\n",
    "\n",
    "There is an underlying toJSON() function that returns an RDD of JSON strings using the column names and schema to produce the JSON records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'{\"id\":\"1\",\"end_date\":\"2018-10-10 00:00:00\",\"start_date\":\"2018-09-14 00:00:00\",\"location\":\"TS-HYD\",\"date\":\"2018-10-10\",\"date_only\":\"2018-10-10\",\"city\":\"HYD\",\"date_diff\":26}',\n",
       " u'{\"id\":\"2\",\"end_date\":\"2018-10-11 01:00:20\",\"start_date\":\"2018-08-14 00:00:00\",\"location\":\"KA-BLR\",\"date\":\"2018-10-11\",\"date_only\":\"2018-10-11\",\"city\":\"BLR\",\"date_diff\":58}']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_json = df.\n",
    "rdd_json.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF\n",
    "### My UDF takes a parameter including the column to operate on. How do I pass this parameter?\n",
    "There is a function available called lit() that creates a constant column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10| HYD|       26|     1001|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11| BLR|       58|     1002|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|2018-10-12|2018-10-12| CHE|      271|     1003|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|2018-10-13|2018-10-13| MUM|      241|     1004|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|2018-10-14|2018-10-14| HYD|      183|     1005|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "add_n = udf(lambda x, y: x + y, IntegerType())\n",
    "\n",
    "# We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.\n",
    "df = df.withColumn('id_offset', add_n(F.lit(1000), df.id.cast(IntegerType())))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10| HYD|       26|     1001|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11| BLR|       58|     1002|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any constants used by UDF will automatically pass through to workers\n",
    "# Write udf that retrieves only the rows where date_diff < 90\n",
    "N = 90\n",
    "last_n_days = \n",
    "\n",
    "df_filtered = df.\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Hive metastore and hive table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "|Stock_Date|Open      |High      |Low       |Close     |Adj_Close|Volume  |\n",
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "|2004-08-25|149.837006|149.837006|122.375   |123.494003|67.378128|136928  |\n",
      "|2004-08-26|124.0     |124.625   |121.912003|122.375   |66.767601|40443200|\n",
      "|2004-08-27|122.800003|122.800003|119.82    |120.332001|65.652962|30646000|\n",
      "|2004-08-30|121.237999|123.75    |120.625   |123.345001|67.296806|24465208|\n",
      "|2004-08-31|123.311996|123.75    |122.0     |123.512001|67.387932|21194656|\n",
      "|2004-09-01|123.75    |124.375   |122.949997|123.487999|67.374817|19935544|\n",
      "|2004-09-02|123.737999|125.574997|123.25    |124.207001|67.767113|21356352|\n",
      "|2004-09-03|125.75    |137.5     |123.794998|124.732002|68.053581|9869856 |\n",
      "|2004-09-06|129.988007|129.988007|124.112999|124.357002|67.848961|9038672 |\n",
      "|2004-09-07|129.375   |129.375   |124.375   |124.449997|67.899704|5772232 |\n",
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Both return DataFrame types\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a DataFrame for the hive table \"tcs_stocks\" using spark.table option\n",
    "df_hive_table_1 = spark.table(\"tcs_stocks_jayantm\")\n",
    "df_hive_table_1.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "|Stock_Date|Open      |High      |Low       |Close     |Adj_Close|Volume  |\n",
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "|2004-08-25|149.837006|149.837006|122.375   |123.494003|67.378128|136928  |\n",
      "|2004-08-26|124.0     |124.625   |121.912003|122.375   |66.767601|40443200|\n",
      "|2004-08-27|122.800003|122.800003|119.82    |120.332001|65.652962|30646000|\n",
      "|2004-08-30|121.237999|123.75    |120.625   |123.345001|67.296806|24465208|\n",
      "|2004-08-31|123.311996|123.75    |122.0     |123.512001|67.387932|21194656|\n",
      "|2004-09-01|123.75    |124.375   |122.949997|123.487999|67.374817|19935544|\n",
      "|2004-09-02|123.737999|125.574997|123.25    |124.207001|67.767113|21356352|\n",
      "|2004-09-03|125.75    |137.5     |123.794998|124.732002|68.053581|9869856 |\n",
      "|2004-09-06|129.988007|129.988007|124.112999|124.357002|67.848961|9038672 |\n",
      "|2004-09-07|129.375   |129.375   |124.375   |124.449997|67.899704|5772232 |\n",
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the hive table \"tcs_stocks\" using spark.sql select operation          \n",
    "df_hive_table_2 = spark.sql(\"SELECT * FROM tcs_stocks_jayantm\")\n",
    "df_hive_table_2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute aggregates on columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+--------------+\n",
      "|location|min(id)|count(id)|avg(date_diff)|\n",
      "+--------+-------+---------+--------------+\n",
      "|  KA-BLR|      2|        1|          58.0|\n",
      "|  MH-MUM|      4|        1|         241.0|\n",
      "|  TS-HYD|      1|        2|         104.5|\n",
      "|  TN-CHE|      3|        1|         271.0|\n",
      "+--------+-------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Provide the min, count, and avg and groupBy the location column. Diplay the results\n",
    "agg_df = df.\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the DataFrames to Parquet, but would like to partition on a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+---------+--------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|end_month|end_year|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+---------+--------+\n",
      "|  1|2018-10-10 00:00:00|2018-09-14 00:00:00|  TS-HYD|2018-10-10|2018-10-10| HYD|       26|     1001|       10|    2018|\n",
      "|  2|2018-10-11 01:00:20|2018-08-14 00:00:00|  KA-BLR|2018-10-11|2018-10-11| BLR|       58|     1002|       10|    2018|\n",
      "|  3|2018-10-12 02:30:00|2018-01-14 00:00:00|  TN-CHE|2018-10-12|2018-10-12| CHE|      271|     1003|       10|    2018|\n",
      "|  4|2018-10-13 03:00:20|2018-02-14 00:00:00|  MH-MUM|2018-10-13|2018-10-13| MUM|      241|     1004|       10|    2018|\n",
      "|  5|2018-10-14 04:30:00|2018-04-14 00:00:00|  TS-HYD|2018-10-14|2018-10-14| HYD|      183|     1005|       10|    2018|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Derive two new columns end_month and end_year where\n",
    "# end_month holds only the month part from end_date column and\n",
    "# end_year holds only the year part from end_date column\n",
    "df = df.\n",
    "df = df.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write above DataFrame to a parquet file partitioned by end_year and end_month\n",
    "# Remove the file(directory) if it exists\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "## delete only if file exists ##\n",
    "filepath = \"/home/<user-id>/tmp/spark_df_example2.parquet\"\n",
    "filename = \"file:///home/<user-id>/tmp/spark_df_example2.parquet/\"\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    shutil.rmtree(filepath)\n",
    "\n",
    "df.write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_year=2018  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/<user-id>/tmp/spark_df_example2.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_month=10\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/<user-id>/tmp/spark_df_example2.parquet/end_year\\=2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-4273a79d-b6e2-4eb0-ba05-a137356c2010.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/<user-id>/tmp/spark_df_example2.parquet/end_year\\=2018/end_month\\=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+--------+---------+\n",
      "|id |end_date           |start_date         |location|date      |date_only |city|date_diff|id_offset|end_year|end_month|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+--------+---------+\n",
      "|1  |2018-10-10 00:00:00|2018-09-14 00:00:00|TS-HYD  |2018-10-10|2018-10-10|HYD |26       |1001     |2018    |10       |\n",
      "|2  |2018-10-11 01:00:20|2018-08-14 00:00:00|KA-BLR  |2018-10-11|2018-10-11|BLR |58       |1002     |2018    |10       |\n",
      "|3  |2018-10-12 02:30:00|2018-01-14 00:00:00|TN-CHE  |2018-10-12|2018-10-12|CHE |271      |1003     |2018    |10       |\n",
      "|4  |2018-10-13 03:00:20|2018-02-14 00:00:00|MH-MUM  |2018-10-13|2018-10-13|MUM |241      |1004     |2018    |10       |\n",
      "|5  |2018-10-14 04:30:00|2018-04-14 00:00:00|TS-HYD  |2018-10-14|2018-10-14|HYD |183      |1005     |2018    |10       |\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF2 = spark.read.parquet(filename)\n",
    "parquetDF2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle cases where I want to filter out NULL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|test|   1|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_item_schema = StructType([StructField(\"col1\", StringType(), True),\n",
    "                               StructField(\"col2\", IntegerType(), True)])\n",
    "\n",
    "null_df = spark.createDataFrame([(\"test\", 1), (None, 2)], null_item_schema)\n",
    "null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|test|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract only the data where col1 value is NOT NULL\n",
    "null_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer the schema using the CSV or spark-avro libraries\n",
    "\n",
    "Read csv file \"df_sample_2.csv\" and inferschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- end_date: timestamp (nullable = true)\n",
      " |-- start_date: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for df_sample_2.csv file and inferschema\n",
    "sample_df_2 = \n",
    "\n",
    "sample_df_2.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
