{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fradulent Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED =7124\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data and lookng at the structure of the data. This data has 30 attributes and 1 lakh records\n",
    "data = pd.read_csv(\"C:\\\\Piazza\\\\Activity Lab\\\\DeepLearning_Autoencoders\\\\Fraud_data_amtstd.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.836500</td>\n",
       "      <td>-0.545419</td>\n",
       "      <td>-0.462979</td>\n",
       "      <td>0.537174</td>\n",
       "      <td>-0.426143</td>\n",
       "      <td>-0.100606</td>\n",
       "      <td>-0.584764</td>\n",
       "      <td>-0.103956</td>\n",
       "      <td>2.268429</td>\n",
       "      <td>-0.365185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085111</td>\n",
       "      <td>0.410736</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.602906</td>\n",
       "      <td>-0.350260</td>\n",
       "      <td>0.464407</td>\n",
       "      <td>-0.070917</td>\n",
       "      <td>-0.030486</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.289880</td>\n",
       "      <td>-2.576061</td>\n",
       "      <td>-0.092256</td>\n",
       "      <td>1.976405</td>\n",
       "      <td>2.810033</td>\n",
       "      <td>-2.669128</td>\n",
       "      <td>-0.981883</td>\n",
       "      <td>-0.470310</td>\n",
       "      <td>-0.025692</td>\n",
       "      <td>0.099528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473240</td>\n",
       "      <td>-0.307295</td>\n",
       "      <td>-2.789549</td>\n",
       "      <td>0.578976</td>\n",
       "      <td>-0.837979</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.353451</td>\n",
       "      <td>-1.662202</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.131318</td>\n",
       "      <td>0.139818</td>\n",
       "      <td>0.586921</td>\n",
       "      <td>1.069291</td>\n",
       "      <td>-0.334908</td>\n",
       "      <td>-0.204938</td>\n",
       "      <td>-0.135526</td>\n",
       "      <td>0.043821</td>\n",
       "      <td>-0.121117</td>\n",
       "      <td>0.182139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028126</td>\n",
       "      <td>-0.167062</td>\n",
       "      <td>-0.048054</td>\n",
       "      <td>-0.009912</td>\n",
       "      <td>0.417694</td>\n",
       "      <td>-0.479793</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>-0.208963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866956</td>\n",
       "      <td>1.373947</td>\n",
       "      <td>1.948343</td>\n",
       "      <td>2.686750</td>\n",
       "      <td>-0.366790</td>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.278349</td>\n",
       "      <td>0.739536</td>\n",
       "      <td>-1.655955</td>\n",
       "      <td>0.708396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>-0.070619</td>\n",
       "      <td>-0.080307</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.159131</td>\n",
       "      <td>0.157940</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.842670</td>\n",
       "      <td>1.401843</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>1.070402</td>\n",
       "      <td>0.843883</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>0.366716</td>\n",
       "      <td>0.616739</td>\n",
       "      <td>-1.586963</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036573</td>\n",
       "      <td>-0.182581</td>\n",
       "      <td>-0.226834</td>\n",
       "      <td>-1.029794</td>\n",
       "      <td>-0.118762</td>\n",
       "      <td>-0.228960</td>\n",
       "      <td>-0.024250</td>\n",
       "      <td>0.046547</td>\n",
       "      <td>-0.346230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.178458</td>\n",
       "      <td>0.166055</td>\n",
       "      <td>-0.101567</td>\n",
       "      <td>0.369453</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>0.396639</td>\n",
       "      <td>-0.187978</td>\n",
       "      <td>-0.483147</td>\n",
       "      <td>0.083094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323048</td>\n",
       "      <td>-1.083814</td>\n",
       "      <td>0.049838</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>0.295810</td>\n",
       "      <td>0.135883</td>\n",
       "      <td>-0.074191</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.150527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.869017</td>\n",
       "      <td>-0.202287</td>\n",
       "      <td>-0.218739</td>\n",
       "      <td>1.496434</td>\n",
       "      <td>-0.403332</td>\n",
       "      <td>-0.013593</td>\n",
       "      <td>-0.342586</td>\n",
       "      <td>0.129402</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.149568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458963</td>\n",
       "      <td>-1.058509</td>\n",
       "      <td>0.439679</td>\n",
       "      <td>-0.066668</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-1.125226</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>-0.039380</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.335053</td>\n",
       "      <td>0.331464</td>\n",
       "      <td>-2.057763</td>\n",
       "      <td>-0.346175</td>\n",
       "      <td>2.583234</td>\n",
       "      <td>2.854102</td>\n",
       "      <td>-0.187547</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>-0.286614</td>\n",
       "      <td>-0.535903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191820</td>\n",
       "      <td>-0.650118</td>\n",
       "      <td>-0.114069</td>\n",
       "      <td>0.915936</td>\n",
       "      <td>0.730073</td>\n",
       "      <td>0.383879</td>\n",
       "      <td>-0.031902</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.787763</td>\n",
       "      <td>-0.737892</td>\n",
       "      <td>-0.185794</td>\n",
       "      <td>0.362758</td>\n",
       "      <td>-0.550775</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>-0.932369</td>\n",
       "      <td>0.390445</td>\n",
       "      <td>1.349983</td>\n",
       "      <td>-0.136095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331025</td>\n",
       "      <td>1.223539</td>\n",
       "      <td>0.264791</td>\n",
       "      <td>-0.541170</td>\n",
       "      <td>-0.571339</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.232691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.055540</td>\n",
       "      <td>0.942471</td>\n",
       "      <td>0.986697</td>\n",
       "      <td>1.560551</td>\n",
       "      <td>-0.138755</td>\n",
       "      <td>-0.253645</td>\n",
       "      <td>0.622974</td>\n",
       "      <td>-0.321826</td>\n",
       "      <td>-0.215914</td>\n",
       "      <td>0.816454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.985031</td>\n",
       "      <td>0.204281</td>\n",
       "      <td>0.455561</td>\n",
       "      <td>-0.456576</td>\n",
       "      <td>-0.244140</td>\n",
       "      <td>-0.833487</td>\n",
       "      <td>-0.419773</td>\n",
       "      <td>-0.173549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.836500 -0.545419 -0.462979  0.537174 -0.426143 -0.100606 -0.584764   \n",
       "1 -4.289880 -2.576061 -0.092256  1.976405  2.810033 -2.669128 -0.981883   \n",
       "2  1.131318  0.139818  0.586921  1.069291 -0.334908 -0.204938 -0.135526   \n",
       "3 -0.866956  1.373947  1.948343  2.686750 -0.366790  0.568632 -0.278349   \n",
       "4 -0.842670  1.401843  0.927235  1.070402  0.843883  0.467333  0.366716   \n",
       "5  1.178458  0.166055 -0.101567  0.369453  0.017198 -0.722891  0.396639   \n",
       "6  1.869017 -0.202287 -0.218739  1.496434 -0.403332 -0.013593 -0.342586   \n",
       "7  1.335053  0.331464 -2.057763 -0.346175  2.583234  2.854102 -0.187547   \n",
       "8  1.787763 -0.737892 -0.185794  0.362758 -0.550775  0.676564 -0.932369   \n",
       "9 -1.055540  0.942471  0.986697  1.560551 -0.138755 -0.253645  0.622974   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0 -0.103956  2.268429 -0.365185  ...    0.085111  0.410736  0.137625   \n",
       "1 -0.470310 -0.025692  0.099528  ...   -0.473240 -0.307295 -2.789549   \n",
       "2  0.043821 -0.121117  0.182139  ...   -0.028126 -0.167062 -0.048054   \n",
       "3  0.739536 -1.655955  0.708396  ...    0.022719 -0.070619 -0.080307   \n",
       "4  0.616739 -1.586963  0.000041  ...    0.036573 -0.182581 -0.226834   \n",
       "5 -0.187978 -0.483147  0.083094  ...   -0.323048 -1.083814  0.049838   \n",
       "6  0.129402  0.911017  0.149568  ...   -0.458963 -1.058509  0.439679   \n",
       "7  0.685154 -0.286614 -0.535903  ...   -0.191820 -0.650118 -0.114069   \n",
       "8  0.390445  1.349983 -0.136095  ...    0.331025  1.223539  0.264791   \n",
       "9 -0.321826 -0.215914  0.816454  ...    0.153985  0.985031  0.204281   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  Class  \n",
       "0  0.602906 -0.350260  0.464407 -0.070917 -0.030486  0.049882      0  \n",
       "1  0.578976 -0.837979  0.372843  0.353451 -1.662202 -0.347171      0  \n",
       "2 -0.009912  0.417694 -0.479793  0.024360  0.023878 -0.208963      0  \n",
       "3  0.000816  0.092167  0.159131  0.157940 -0.014370 -0.253595      0  \n",
       "4 -1.029794 -0.118762 -0.228960 -0.024250  0.046547 -0.346230      0  \n",
       "5 -0.002872  0.295810  0.135883 -0.074191  0.004364 -0.150527      0  \n",
       "6 -0.066668 -0.376792 -1.125226  0.053537 -0.039380 -0.305050      0  \n",
       "7  0.915936  0.730073  0.383879 -0.031902  0.029849 -0.347171      0  \n",
       "8 -0.541170 -0.571339  0.812785  0.017984 -0.054691 -0.232691      0  \n",
       "9  0.455561 -0.456576 -0.244140 -0.833487 -0.419773 -0.173549      0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at a sample of records\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    99508\n",
      "1      492\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the frequencies of fraud and non-fraud transactions in the data\n",
    "count_classes = pd.value_counts(data['Class'], sort = True)\n",
    "print(count_classes)\n",
    "\n",
    "#Drawing a barplot\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Converting data to array\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 30)\n",
      "(20000, 30)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train and test and observing their dimensions\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([79591,   409], dtype=int64))\n",
      "(array([0., 1.]), array([19917,    83], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the fraud and non-fraud records in train\n",
    "print(np.unique(X_train[:,29],return_counts=True))\n",
    "print(np.unique(X_test[:,29],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79591, 29)\n"
     ]
    }
   ],
   "source": [
    "#Now consider only the non-fraud records for training\n",
    "X_train_NF = X_train[X_train[:,-1] == 0]\n",
    "X_train_NF = X_train_NF[:,:-1]\n",
    "print(X_train_NF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Separating out the fraud records from the train \n",
    "X_train_F = X_train[X_train[:,-1] == 1]\n",
    "print(X_train_F.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Adding/concatenating the fraud records from train data to the test\n",
    "X_test=np.concatenate((X_test,X_train_F),axis=0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test,X_eval = train_test_split(X_test, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16327, 30)\n",
      "(4082, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating the independent and the class variable\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16327, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expanding the dimensions of y for later concatenation\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train_NF.shape[1]\n",
    "encoding_dim = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "autoencoder.add(Dense(input_dim, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bharg\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.6953 - mean_squared_error: 0.6953 - val_loss: 0.3735 - val_mean_squared_error: 0.3735\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.4200 - mean_squared_error: 0.4200 - val_loss: 0.2723 - val_mean_squared_error: 0.2723\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3728 - mean_squared_error: 0.3728 - val_loss: 0.2464 - val_mean_squared_error: 0.2464\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.2353 - val_mean_squared_error: 0.2353\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.2352 - val_mean_squared_error: 0.2352\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3466 - mean_squared_error: 0.3466 - val_loss: 0.2317 - val_mean_squared_error: 0.2317\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3495 - mean_squared_error: 0.3495 - val_loss: 0.2301 - val_mean_squared_error: 0.2301\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3506 - mean_squared_error: 0.3506 - val_loss: 0.2336 - val_mean_squared_error: 0.2336\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3500 - mean_squared_error: 0.3500 - val_loss: 0.2414 - val_mean_squared_error: 0.2414\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3442 - mean_squared_error: 0.3442 - val_loss: 0.2354 - val_mean_squared_error: 0.2354\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3462 - mean_squared_error: 0.3462 - val_loss: 0.2273 - val_mean_squared_error: 0.2273\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3469 - mean_squared_error: 0.3469 - val_loss: 0.2314 - val_mean_squared_error: 0.2314\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3473 - mean_squared_error: 0.3473 - val_loss: 0.2287 - val_mean_squared_error: 0.2287\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3446 - mean_squared_error: 0.3446 - val_loss: 0.2293 - val_mean_squared_error: 0.2293\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3466 - mean_squared_error: 0.3466 - val_loss: 0.2307 - val_mean_squared_error: 0.2307\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3488 - mean_squared_error: 0.3488 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3418 - mean_squared_error: 0.3418 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3406 - mean_squared_error: 0.3406 - val_loss: 0.2273 - val_mean_squared_error: 0.2273\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3417 - mean_squared_error: 0.3417 - val_loss: 0.2266 - val_mean_squared_error: 0.2266\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3437 - mean_squared_error: 0.3437 - val_loss: 0.2245 - val_mean_squared_error: 0.2245\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3396 - mean_squared_error: 0.3396 - val_loss: 0.2261 - val_mean_squared_error: 0.2261\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3405 - mean_squared_error: 0.3405 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3436 - mean_squared_error: 0.3436 - val_loss: 0.2253 - val_mean_squared_error: 0.2253\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3413 - mean_squared_error: 0.3413 - val_loss: 0.2280 - val_mean_squared_error: 0.2280\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3422 - mean_squared_error: 0.3422 - val_loss: 0.2249 - val_mean_squared_error: 0.2249\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3382 - mean_squared_error: 0.3382 - val_loss: 0.2277 - val_mean_squared_error: 0.2277\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3404 - mean_squared_error: 0.3404 - val_loss: 0.2259 - val_mean_squared_error: 0.2259\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3365 - mean_squared_error: 0.3365 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2269 - val_mean_squared_error: 0.2269\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3358 - mean_squared_error: 0.3358 - val_loss: 0.2282 - val_mean_squared_error: 0.2282\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3376 - mean_squared_error: 0.3376 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3384 - mean_squared_error: 0.3384 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3375 - mean_squared_error: 0.3375 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3384 - mean_squared_error: 0.3384 - val_loss: 0.2264 - val_mean_squared_error: 0.2264\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2251 - val_mean_squared_error: 0.2251\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3346 - mean_squared_error: 0.3346 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2235 - val_mean_squared_error: 0.2235\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3334 - mean_squared_error: 0.3334 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3361 - mean_squared_error: 0.3361 - val_loss: 0.2298 - val_mean_squared_error: 0.2298\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3345 - mean_squared_error: 0.3345 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3368 - mean_squared_error: 0.3368 - val_loss: 0.2264 - val_mean_squared_error: 0.2264\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3391 - mean_squared_error: 0.3391 - val_loss: 0.2302 - val_mean_squared_error: 0.2302\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3350 - mean_squared_error: 0.3350 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3348 - mean_squared_error: 0.3348 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3359 - mean_squared_error: 0.3359 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2237 - val_mean_squared_error: 0.2237\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3376 - mean_squared_error: 0.3376 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3347 - mean_squared_error: 0.3347 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3325 - mean_squared_error: 0.3325 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3351 - mean_squared_error: 0.3351 - val_loss: 0.2266 - val_mean_squared_error: 0.2266\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3355 - mean_squared_error: 0.3355 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3299 - mean_squared_error: 0.3299 - val_loss: 0.2249 - val_mean_squared_error: 0.2249\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3321 - mean_squared_error: 0.3321 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3314 - mean_squared_error: 0.3314 - val_loss: 0.2257 - val_mean_squared_error: 0.2257\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.2270 - val_mean_squared_error: 0.2270\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3334 - mean_squared_error: 0.3334 - val_loss: 0.2282 - val_mean_squared_error: 0.2282\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3323 - mean_squared_error: 0.3323 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3332 - mean_squared_error: 0.3332 - val_loss: 0.2300 - val_mean_squared_error: 0.2300\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3324 - mean_squared_error: 0.3324 - val_loss: 0.2277 - val_mean_squared_error: 0.2277\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.2268 - val_mean_squared_error: 0.2268\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3339 - mean_squared_error: 0.3339 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.2253 - val_mean_squared_error: 0.2253\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3327 - mean_squared_error: 0.3327 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3333 - mean_squared_error: 0.3333 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3305 - mean_squared_error: 0.3305 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2268 - val_mean_squared_error: 0.2268\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3302 - mean_squared_error: 0.3302 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3293 - mean_squared_error: 0.3293 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3268 - mean_squared_error: 0.3268 - val_loss: 0.2225 - val_mean_squared_error: 0.2225\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3271 - mean_squared_error: 0.3271 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3281 - mean_squared_error: 0.3281 - val_loss: 0.2243 - val_mean_squared_error: 0.2243\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2243 - val_mean_squared_error: 0.2243\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3295 - mean_squared_error: 0.3295 - val_loss: 0.2262 - val_mean_squared_error: 0.2262\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3283 - mean_squared_error: 0.3283 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55713/55713 [==============================] - 2s - loss: 0.3322 - mean_squared_error: 0.3322 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3260 - mean_squared_error: 0.3260 - val_loss: 0.2225 - val_mean_squared_error: 0.2225\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3269 - mean_squared_error: 0.3269 - val_loss: 0.2227 - val_mean_squared_error: 0.2227\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3258 - mean_squared_error: 0.3258 - val_loss: 0.2224 - val_mean_squared_error: 0.2224\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3294 - mean_squared_error: 0.3294 - val_loss: 0.2244 - val_mean_squared_error: 0.2244\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3291 - mean_squared_error: 0.3291 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3271 - mean_squared_error: 0.3271 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3256 - mean_squared_error: 0.3256 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3291 - mean_squared_error: 0.3291 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3329 - mean_squared_error: 0.3329 - val_loss: 0.2261 - val_mean_squared_error: 0.2261\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3278 - mean_squared_error: 0.3278 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3278 - mean_squared_error: 0.3278 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3270 - mean_squared_error: 0.3270 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3273 - mean_squared_error: 0.3273 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3288 - mean_squared_error: 0.3288 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3301 - mean_squared_error: 0.3301 - val_loss: 0.2254 - val_mean_squared_error: 0.2254\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3295 - mean_squared_error: 0.3295 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3259 - mean_squared_error: 0.3259 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for _ in range(100):\n",
    "    hist.append(autoencoder.fit(X_train_NF, X_train_NF,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.3,\n",
    "                    verbose=1).history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': [0.695324907640844],\n",
       "  'mean_squared_error': [0.695324907640844],\n",
       "  'val_loss': [0.37349971829447],\n",
       "  'val_mean_squared_error': [0.37349971829447]},\n",
       " {'loss': [0.42003668715649006],\n",
       "  'mean_squared_error': [0.42003668715649006],\n",
       "  'val_loss': [0.2723358369654163],\n",
       "  'val_mean_squared_error': [0.2723358369654163]},\n",
       " {'loss': [0.3727933886126581],\n",
       "  'mean_squared_error': [0.3727933886126581],\n",
       "  'val_loss': [0.2464360946572369],\n",
       "  'val_mean_squared_error': [0.2464360946572369]},\n",
       " {'loss': [0.3635741629834747],\n",
       "  'mean_squared_error': [0.3635741629834747],\n",
       "  'val_loss': [0.23530568360386206],\n",
       "  'val_mean_squared_error': [0.23530568360386206]},\n",
       " {'loss': [0.35717580086608725],\n",
       "  'mean_squared_error': [0.35717580086608725],\n",
       "  'val_loss': [0.23517274355077597],\n",
       "  'val_mean_squared_error': [0.23517274355077597]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Making predictions on the train data\n",
    "predictions=autoencoder.predict(X_train_NF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4432399e+00,  8.7036669e-01,  1.6521544e+00,  8.1679034e-01,\n",
       "        -7.3549169e-01,  3.2794514e-01, -5.8283901e-01,  1.2259694e+00,\n",
       "         1.9634995e-01, -1.0537603e+00, -1.3853183e+00,  2.9447156e-01,\n",
       "        -8.4814215e-01, -1.4758524e-01, -1.6575549e+00, -1.6407430e-01,\n",
       "         4.4036913e-01,  9.2143387e-02,  7.4829608e-03,  1.2122378e-01,\n",
       "         9.5195964e-02, -5.8713667e-02, -8.5580945e-03, -7.3304877e-02,\n",
       "        -3.8180023e-02,  1.3117905e-02, -7.5090211e-03,  1.8448427e-02,\n",
       "        -1.4113742e-01],\n",
       "       [ 1.1887827e+00, -1.9675851e-01, -4.7158360e-02, -4.5041168e-01,\n",
       "        -5.8828139e-01, -1.3243284e+00, -3.6417242e-02, -2.4616992e-01,\n",
       "        -1.1154726e+00,  5.6852961e-01,  3.2103717e-01, -1.5510117e-01,\n",
       "         3.5319638e-01, -1.9294584e-01,  3.2146811e-01,  9.6328020e-02,\n",
       "         1.5686655e-01,  4.4948488e-02,  3.6132753e-02, -5.3251833e-02,\n",
       "        -1.6575530e-03, -1.2422707e-03, -6.1567463e-03, -1.0330281e-01,\n",
       "        -1.4341621e-02, -2.1778617e-02, -3.2922309e-02, -1.8280478e-02,\n",
       "        -2.0252237e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16327, 30)\n",
      "(15942, 30)\n",
      "(385, 30)\n"
     ]
    }
   ],
   "source": [
    "##We want to separate out fraud records and non-fraud records for later use\n",
    "f = np.hstack((X_test,y_test))\n",
    "print(f.shape)\n",
    "\n",
    "test_nf=f[f[:,29]==0]\n",
    "print(test_nf.shape)\n",
    "\n",
    "test_f=f[f[:,29]==1]\n",
    "print(test_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14304/15942 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2301862383409241, 0.2301862383409241]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the non fraud data separately \n",
    "autoencoder.evaluate(test_nf[:,:29],test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/385 [=>............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.560293519651736, 19.560293519651736]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the fraud data separately\n",
    "autoencoder.evaluate(test_f[:,:29],test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining predictions for non fraud records\n",
    "predictions_nf=autoencoder.predict(test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining predictions for fraud records\n",
    "predictions_f=autoencoder.predict(test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2301862366657775"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the error computation method by autoencoder(Mean Squared Error). The computation is as follows \n",
    "np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing errors on the non-fraud data\n",
    "errors_nf = np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30963621, 0.05360371, 0.19483006, 0.07043692, 0.40740092])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_nf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing errors on the fraud data\n",
    "errors_f = np.mean(np.square(np.abs(test_f[:,:29]-predictions_f)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89.90100995, 75.02913094, 17.7202158 ,  4.01550371, 71.16703302])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_f[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016708967105815792\n",
      "110.20634003428978\n",
      "0.12419686431327694\n",
      "0.046207987150603376\n",
      "138.1402238710232\n",
      "7.99036979283013\n"
     ]
    }
   ],
   "source": [
    "#Computing the distribution of errors in both non-fraud and fraud data\n",
    "print(np.min(errors_nf))\n",
    "print(np.max(errors_nf))\n",
    "print(np.median(errors_nf))\n",
    "\n",
    "print(np.min(errors_f))\n",
    "print(np.max(errors_f))\n",
    "print(np.median(errors_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x176cbbf6550>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x176cbbfd908>,\n",
       "  <matplotlib.lines.Line2D at 0x176cbbfdf28>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x176cbc09ba8>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x176cbc09080>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x176cbbf6c88>,\n",
       "  <matplotlib.lines.Line2D at 0x176cbbf6da0>]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x176c669ed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the error box plots \n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.boxplot(errors_f)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(errors_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "192\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7971"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Experimentation to fix a threshold for classification of a transaction into fraud or non-fraud\n",
    "print(sum(errors_nf>np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_nf)))\n",
    "sum(errors_nf>np.median(errors_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15942,)\n",
      "(385,)\n"
     ]
    }
   ],
   "source": [
    "print(errors_nf.shape)\n",
    "print(errors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15942, 29)\n",
      "(385, 29)\n"
     ]
    }
   ],
   "source": [
    "print(predictions_nf.shape)\n",
    "print(predictions_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = autoencoder.predict(X_test[:,:29])\n",
    "test_recon  = (((test_pred-X_test)**2).mean(-1))\n",
    "\n",
    "train_pred = autoencoder.predict(X_train_NF[:,:29])\n",
    "mean_recon = (((train_pred - X_train_NF)**2).mean(-1).mean())\n",
    "\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "scores_f1 = []\n",
    "thres = []\n",
    "\n",
    "th = 0\n",
    "for i in range(100):\n",
    "    th+=0.1\n",
    "    fraud = (test_recon>mean_recon+th)\n",
    "    scores_f1.append(f1_score(y_test,fraud))\n",
    "    thres.append(th+mean_recon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16327, 29)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.925861360803925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[15882,    60],\n",
       "       [   89,   296]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x176cbc112b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thres, scores_f1)\n",
    "\n",
    "print(thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "fraud = (test_recon>thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "confusion_matrix(y_test, fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Predicting on Valdation \n",
    "\n",
    "predictions_eval=autoencoder.predict(X_eval[:,:29])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors_eval=np.square(np.subtract(predictions_eval,X_eval[:,:29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraud_eval=(((errors_eval-X_eval[:,:29])**2).mean(-1))>2.925861360803925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3682  293]\n",
      " [  15   92]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=X_eval[:,29],y_pred=fraud_eval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
