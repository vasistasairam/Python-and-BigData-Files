{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Operations using Spark DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this activity we will understand\n",
    "-  What are DataFrames in Spark ?\n",
    "-  Different ways to create a DataFrames\n",
    "-  What are Spark Transformations & Actions\n",
    "-  Verify Summary Statistics\n",
    "-  Spark SQL\n",
    "-  Performance Comparison of Spark DataFrame and Spark SQL\n",
    "-  Column References\n",
    "-  Converting to Spark Types - Literals\n",
    "-  Add/Rename/Remove Columns\n",
    "-  TypeCasting\n",
    "-  Column differences\n",
    "-  Pair-wise frequencies\n",
    "-  Remove duplicates\n",
    "-  Working with Nulls\n",
    "-  Filtering the rows\n",
    "-  Aggregations\n",
    "-  Joins\n",
    "-  Random Samples\n",
    "-  Random Splits\n",
    "-  Map Transformations\n",
    "-  Sorting\n",
    "-  Union\n",
    "-  String Manipulations\n",
    "-  Regular Expressions\n",
    "-  Working with Dates and Time Stamp\n",
    "-  User Defined Functions \n",
    "-  Broadcase variables and Accumulators\n",
    "-  Handling Different Data Sources\n",
    "-  New features available in Spark 2.3 \n",
    "    -  PySpark Usage for Pandas with Apache Arrow\n",
    "        -  Enabling for Conversion to/from Pandas\n",
    "        -  Pandas UDFs (a.k.a. Vectorized UDFs)\n",
    "            - Scalar\n",
    "            - Grouped Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create SparkContext, SparkSession\n",
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = 'hdfs:///apps/hive/warehouse/'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.221:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL Hive integration example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f875b5910d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.2.221:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL Hive integration example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Python Spark SQL Hive integration example>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation\n",
    "- **Pandas** - DataFrames represented on a single machine as Python data structures\n",
    "- **RDDs** - Spark’s foundational structure Resilient Distributed Dataset is represented as a reference to partitioned data without types\n",
    "- **DataFrames** - Spark’s strongly typed optimized distributed collection of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Spark DataFrame **\n",
    "\n",
    "#### A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. \n",
    "<br> The list that defines the columns and the types within those columns is called the schema. \n",
    "<br> One can think of a DataFrame as a spreadsheet with named columns.\n",
    "<br> A spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.\n",
    "<br> The reason for putting the data on more than one computer should be intuitive: \n",
    "<br>     either the data is too large to fit on one machine or \n",
    "<br>     it would simply take too long to perform that computation on one machine.\n",
    "\n",
    "#### NOTE\n",
    "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs). \n",
    "<br> These different abstractions all represent distributed collections of data. \n",
    "<br> The easiest and most efficient are DataFrames, which are available in all languages.\n",
    "\n",
    "![Spark DataFrame](./Images/SparkDataFrame.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with one column containing 100 rows with values from 0 to 99.\n",
    "This range of numbers represents a distributed collection. \n",
    "<br> When run on a cluster, each part of this range of numbers exists on a different executor. \n",
    "<br> This is a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRange = spark.range(100).toDF('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| Id| Name|Age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 30|\n",
      "|  2|  Bob| 28|\n",
      "|  3|Cathy| 31|\n",
      "|  4| Dave| 56|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF = spark.createDataFrame([[1, 'Alice', 30],\n",
    "                              [2, 'Bob', 28],\n",
    "                              [3, 'Cathy', 31], \n",
    "                              [4, 'Dave', 56]], ['Id', 'Name', 'Age'])\n",
    "\n",
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations & Actions\n",
    "\n",
    "### Transformations\n",
    "In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created.\n",
    "<br> To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want.\n",
    "<br> These instructions are called transformations.\n",
    "<br> Transformations are the core of how you express your business logic using Spark.\n",
    "<br> Transformations are simply ways of specifying different series of data manipulation.\n",
    "\n",
    "![Spark Transformations](./Images/Spark_Transformations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "divisBy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these return no output. <br>This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Transformations allow us to build up our logical transformation plan. \n",
    "<br> To trigger the computation, we run an action.\n",
    "<br> An action instructs Spark to compute a result from a series of transformations. \n",
    "<br> The simplest action is count, which gives us the total number of records in the DataFrame:\n",
    "\n",
    "#### There are 3 types of actions\n",
    "Actions to view data in the console\n",
    "<br>Actions to collect data to native objects in the respective language\n",
    "<br>Actions to write to output data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperating with RDDs\n",
    "\n",
    "<br> Spark SQL supports two different methods for converting existing RDDs into DataFrames. \n",
    "<br> The first method uses reflection to infer the schema of an RDD that contains specific types of objects. \n",
    "<br> This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    "\n",
    "<br> The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. \n",
    "<br> While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/insofe/IBM/Day03\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an RDD from a source\n",
    "tempRDD = sc.textFile(\"file:///home/insofe/IBM/Day03/temp_data.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1901\\t-78\\t1', u'1901\\t-72\\t1', u'1901\\t-94\\t1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1901', u'-78', u'1'], [u'1901', u'-72', u'1'], [u'1901', u'-94', u'1']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRDD = tempRDD.map(lambda line: line.split(\"\\t\"))\n",
    "splitRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status=1, temp=-78, year=u'1901'),\n",
       " Row(status=1, temp=-72, year=u'1901'),\n",
       " Row(status=1, temp=-94, year=u'1901')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemafiedRDD = splitRDD.map(lambda line: Row(year=line[0], temp=int(line[1]), status=int(line[2])))\n",
    "schemafiedRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+\n",
      "|status|temp|year|\n",
      "+------+----+----+\n",
      "|     1| -78|1901|\n",
      "|     1| -72|1901|\n",
      "|     1| -94|1901|\n",
      "+------+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "tempDF = spark.createDataFrame(schemafiedRDD)\n",
    "tempDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n",
    "- Create an RDD of tuples or lists from the original RDD;\n",
    "- Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
    "- Apply the schema to the RDD via createDataFrame method provided by SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Records with header: ', 23380)\n",
      "\n",
      "First Two Records Before Removing Header\n",
      "\n",
      "[u'User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3', u'1000029,P00111542,M,36-45,7,C,1,0,2,17,']\n"
     ]
    }
   ],
   "source": [
    "testRDD = sc.textFile(\"file:///home/insofe/IBM/Day03/test_sample10.csv\")\n",
    "print(\"Total Records with header: \", testRDD.count())\n",
    "print(\"\\nFirst Two Records Before Removing Header\\n\")\n",
    "print(testRDD.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Records without header: ', 23379)\n",
      "\n",
      "First Two Records After Removing Header\n",
      "\n",
      "[u'1000029,P00111542,M,36-45,7,C,1,0,2,17,', u'1000034,P00265242,F,18-25,0,A,0,0,5,8,']\n"
     ]
    }
   ],
   "source": [
    "header = testRDD.first()\n",
    "testRDD = testRDD.filter(lambda line: line != header)\n",
    "print(\"Total Records without header: \", testRDD.count())\n",
    "print(\"\\nFirst Two Records After Removing Header\\n\")\n",
    "print(testRDD.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Two Records After Split/Parsing\n",
      "\n",
      "[[u'1000029', u'P00111542', u'M', u'36-45', u'7', u'C', u'1', u'0', u'2', u'17', u''], [u'1000034', u'P00265242', u'F', u'18-25', u'0', u'A', u'0', u'0', u'5', u'8', u'']]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into individual columns\n",
    "splitRDD = testRDD.map(lambda line: line.split(\",\"))\n",
    "print(\"\\nFirst Two Records After Split/Parsing\\n\")\n",
    "print(splitRDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe for the above Data\n",
    "1. Define Schema\n",
    "2. Create dataframe using the above schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "testSchema = StructType([\n",
    "    StructField(\"User_ID\", StringType(), True),\n",
    "    StructField(\"Product_ID\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Age\", StringType(), True),\n",
    "    StructField(\"Occupation\", StringType(), True),\n",
    "    StructField(\"City_Category\", StringType(), True),\n",
    "    StructField(\"Stay_In_Current_City_Years\", StringType(), True),\n",
    "    StructField(\"Marital_Status\", StringType(), True),\n",
    "    StructField(\"Product_Category_1\", StringType(), True),\n",
    "    StructField(\"Product_Category_2\", StringType(), True),\n",
    "    StructField(\"Product_Category_3\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame using the above schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDF = spark.createDataFrame(data = splitRDD, schema=testSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=u'1000029', Product_ID=u'P00111542', Gender=u'M', Age=u'36-45', Occupation=u'7', City_Category=u'C', Stay_In_Current_City_Years=u'1', Marital_Status=u'0', Product_Category_1=u'2', Product_Category_2=u'17', Product_Category_3=u''),\n",
       " Row(User_ID=u'1000034', Product_ID=u'P00265242', Gender=u'F', Age=u'18-25', Occupation=u'0', City_Category=u'A', Stay_In_Current_City_Years=u'0', Marital_Status=u'0', Product_Category_1=u'5', Product_Category_2=u'8', Product_Category_3=u'')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "|1000029| P00111542|     M|36-45|         7|            C|                         1|             0|                 2|                17|                  |\n",
      "|1000034| P00265242|     F|18-25|         0|            A|                         0|             0|                 5|                 8|                  |\n",
      "|1000053|  P0097342|     M|26-35|         0|            B|                         1|             0|                 1|                15|                16|\n",
      "|1000080| P00112142|     M|  55+|         1|            C|                         3|             1|                 1|                 2|                14|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23379"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1000029,P00111542,M,36-45,7,C,1,0,2,17,',\n",
       " u'1000034,P00265242,F,18-25,0,A,0,0,5,8,']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a CSV file into a DataFrame and converting it to a local array or list of rows.\n",
    "\n",
    "\n",
    "![Reading CSV](./Images/csvDataFrame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read data and create the dataframes\n",
    "## trainDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"file:///home/rameshm/Datasets/SalesData/train.csv\")\n",
    "## testDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"file:///home/rameshm/Datasets/SalesData/test.csv\")\n",
    "trainDF = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"file:///home/insofe/IBM/Day03/train_sample10.csv\")\n",
    "        \n",
    "# testDF = spark.read.format(\"csv\")\\\n",
    "#         .option(\"header\", \"true\")\\\n",
    "#         .option(\"inferSchema\", \"true\")\\\n",
    "#         .load(\"file:///home/rameshm/Datasets/SalesData/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6.0, Product_Category_3=14.0, Purchase=15200),\n",
       " Row(User_ID=1000001, Product_ID=u'P00087842', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n",
       " Row(User_ID=1000001, Product_ID=u'P00085442', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14.0, Product_Category_3=None, Purchase=1057),\n",
       " Row(User_ID=1000002, Product_ID=u'P00285442', Gender=u'M', Age=u'55+', Occupation=16, City_Category=u'C', Stay_In_Current_City_Years=u'4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: double (nullable = true)\n",
      " |-- Product_Category_3: double (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print Schema\n",
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6.0, Product_Category_3=14.0, Purchase=15200)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To Show first n observations\n",
    "## Use head operation to see first n observations (say, 2 observations). \n",
    "## Head operation in PySpark is similar to head operation in Pandas.\n",
    "trainDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Above results are comprised of row like format. \n",
    "## To see the result in more interactive manner (rows under the columns), Use the show operation. \n",
    "## Show operation on train and take first 5 rows of it. \n",
    "trainDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records count in train dataset is 550068\n",
      "Total records count in test dataset is 23379\n"
     ]
    }
   ],
   "source": [
    "## To Count the number of rows in DataFrame\n",
    "print('Total records count in train dataset is {}'.format(trainDF.count()))\n",
    "print('Total records count in test dataset is {}'.format(testDF.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns count in train dataset is 12\n",
      "\n",
      "\n",
      "Columns in train dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase'] \n",
      "\n",
      "Total Columns count in test dataset is 11\n",
      "\n",
      "\n",
      "Columns in test dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Columns count and column names\n",
    "print(\"Total Columns count in train dataset is {}\".format(len(trainDF.columns)))\n",
    "print(\"\\n\\nColumns in train dataset are: {} \\n\".format(trainDF.columns))\n",
    "\n",
    "print(\"Total Columns count in test dataset is {}\".format(len(testDF.columns)))\n",
    "print(\"\\n\\nColumns in test dataset are: {} \\n\".format(testDF.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
      "| stddev| 1727.591585530636|      null|  null|  null|6.522660487341751|         null|        0.9890866807573153|0.49177012631732797|3.9362113692013727| 5.086589648693497| 4.125337631575269|5023.065393820567|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|               2.0|               3.0|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|              18.0|              18.0|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame\n",
    "trainDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         Purchase|\n",
      "+-------+-----------------+\n",
      "|  count|           550068|\n",
      "|   mean|9263.968712959126|\n",
      "| stddev|5023.065393820567|\n",
      "|    min|               12|\n",
      "|    max|            23961|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check what happens when we specify the name of a categorical / String columns in describe operation.\n",
    "## describe operation is working for String type column but the output for mean, stddev are null and \n",
    "## min & max values are calculated based on ASCII value of categories.\n",
    "trainDF.describe('Purchase').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n",
    "<br>There is no performance difference between writing SQL queries or writing DataFrame code, <br>they both “compile” to the same underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create view/table\n",
    "trainDF.createOrReplaceTempView(\"trainDFTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Dataframe\n",
    "trainDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6.0, Product_Category_3=14.0, Purchase=15200)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify Dataframe\n",
    "trainDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Table\n",
    "spark.sql(\"SELECT * FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Comparison Spark DataFrame vs Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#121], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#121, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#121], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#121] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/insofe/IBM/Day03/train_sample10.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "#dataframeWay = trainDF.where(trainDF.Purchase>15000).count()\n",
    "dataframeWay = trainDF.groupBy('Age').count()\n",
    "dataframeWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#121], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#121, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#121], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#121] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/insofe/IBM/Day03/train_sample10.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"SELECT Age, count(1) FROM trainDFTable GROUP BY Age\")\n",
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select & SelectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "| userID|User_ID|User_ID|User_ID|\n",
      "+-------+-------+-------+-------+\n",
      "|1000001|1000001|1000001|1000001|\n",
      "|1000001|1000001|1000001|1000001|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Multiple ways of referring a column in a dataframe\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "trainDF.select(expr(\"User_ID AS userID\") , \n",
    "               col(\"User_ID\"), \n",
    "               column(\"User_ID\"), \"User_ID\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[User_ID: int, User_ID: int]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select(col(\"User_ID\"), \"User_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dot notation doesn't work here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = trainDF.User_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save/assign a column name to the newly created variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User_ID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select content from the above column\n",
    "trainDF.select(result).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(expr(\"User_ID AS userID\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT User_ID AS userID FROM trainDFTable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "| userID|productID|\n",
      "+-------+---------+\n",
      "|1000001|P00069042|\n",
      "|1000001|P00248942|\n",
      "+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.selectExpr(\"User_ID AS userID\", \"Product_ID AS productID\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+\n",
      "|User_ID|Product_ID| Age|\n",
      "+-------+----------+----+\n",
      "|1000001| P00069042|0-17|\n",
      "|1000001| P00248942|0-17|\n",
      "+-------+----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"User_ID\", \"Product_ID\", \"Age\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to Spark Types (Literals)\n",
    "Sometimes we need to pass explicit values into Spark that aren’t a new column but are just a value in all the rows. This might be a constant value or something we’ll need to compare to later on. The way we do this is through literals. \n",
    "This is basically a translation from a given programming language’s literal value to one that Spark understands. \n",
    "Literals are expressions and can be used in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "trainDF.select(\"*\", lit(1).alias('One')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In SQL, literals are just the specific value.\n",
    "spark.sql(\"SELECT *, 1 as One FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## More Formal way\n",
    "trainDF.withColumn(\"One\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *, 1 AS One FROM trainDFTable LIMIT 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|SameCategoryCode|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|            null|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|           false|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|            null|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|              14.0|              null|    1057|           false|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF = trainDF.withColumn(\"SameCategoryCode\", \n",
    "trainDF[\"Product_Category_1\"] == trainDF[\"Product_Category_2\"])\n",
    "tempDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|SimilarCategory|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|           null|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|          false|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.withColumnRenamed(\"SameCategoryCode\", \"SimilarCategory\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.drop(\"SameCategoryCode\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing a Column’s Type (cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: double (nullable = true)\n",
      " |-- Product_Category_3: double (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- SameCategoryCode: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: double (nullable = true)\n",
      " |-- Product_Category_3: double (nullable = true)\n",
      " |-- Purchase: string (nullable = true)\n",
      " |-- SameCategoryCode: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.withColumn(\"Purchase\", col(\"Purchase\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in Product_ID's in train dataset are 3631\n",
      "Distinct values in Product_ID's in test dataset are 2762\n"
     ]
    }
   ],
   "source": [
    "## To find the number of distinct product in train and test datasets\n",
    "## To calculate the number of distinct products in train and test datasets apply distinct operation.\n",
    "print(\"Distinct values in Product_ID's in train dataset are {}\".format(trainDF.select('Product_ID').distinct().count()))\n",
    "print(\"Distinct values in Product_ID's in test dataset are {}\".format(testDF.select('Product_ID').distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Product_ID's there in test dataset but not train dataset are 5\n",
      "Count of Product_ID's there in train dataset but not test dataset are 874\n"
     ]
    }
   ],
   "source": [
    "## From the above we can see the train file has more categories than test file. \n",
    "## Let us check what are the categories for Product_ID, which are in test file but not in train file by \n",
    "## applying subtract operation.\n",
    "## We can do the same for all categorical features.\n",
    "diff_cat_in_test_train=testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "print(\"Count of Product_ID's there in test dataset but not train dataset are {}\".format(diff_cat_in_test_train.count()))\n",
    "\n",
    "diff_cat_in_train_test=trainDF.select('Product_ID').subtract(testDF.select('Product_ID'))\n",
    "print(\"Count of Product_ID's there in train dataset but not test dataset are {}\".format(diff_cat_in_train_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair wise Frequencies - Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|Age_Gender|    F|     M|\n",
      "+----------+-----+------+\n",
      "|      0-17| 5083| 10019|\n",
      "|     46-50|13199| 32502|\n",
      "|     18-25|24628| 75032|\n",
      "|     36-45|27170| 82843|\n",
      "|       55+| 5083| 16421|\n",
      "|     51-55| 9894| 28607|\n",
      "|     26-35|50752|168835|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To calculate pair wise frequency of categorical columns\n",
    "## Use crosstab operation on DataFrame to calculate the pair wise frequency of columns. \n",
    "## Apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame.\n",
    "trainDF.crosstab('Age', 'Gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|  Age|Gender| count|\n",
      "+-----+------+------+\n",
      "|51-55|     F|  9894|\n",
      "|18-25|     M| 75032|\n",
      "| 0-17|     F|  5083|\n",
      "|46-50|     M| 32502|\n",
      "|18-25|     F| 24628|\n",
      "|  55+|     M| 16421|\n",
      "|  55+|     F|  5083|\n",
      "|36-45|     M| 82843|\n",
      "|26-35|     F| 50752|\n",
      "| 0-17|     M| 10019|\n",
      "|36-45|     F| 27170|\n",
      "|51-55|     M| 28607|\n",
      "|26-35|     M|168835|\n",
      "|46-50|     F| 13199|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy('Age', 'Gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|  Age|    F|     M|\n",
      "+-----+-----+------+\n",
      "|18-25|24628| 75032|\n",
      "|26-35|50752|168835|\n",
      "| 0-17| 5083| 10019|\n",
      "|46-50|13199| 32502|\n",
      "|51-55| 9894| 28607|\n",
      "|36-45|27170| 82843|\n",
      "|  55+| 5083| 16421|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select Age,\n",
    "    sum(case when Gender = 'F' then 1 else 0 end) F,\n",
    "    sum(case when Gender = 'M' then 1 else 0 end) M\n",
    "from trainDFTable\n",
    "group by Age\"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"select Age,\n",
    "#     count(*) total,\n",
    "#     sum(case when Gender = 'F' then 1 else 0 end) F,\n",
    "#     sum(case when Gender = 'M' then 1 else 0 end) M\n",
    "# from trainDFTable\n",
    "# group by Age\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Age|Gender|\n",
      "+-----+------+\n",
      "|51-55|     F|\n",
      "|18-25|     M|\n",
      "| 0-17|     F|\n",
      "|46-50|     M|\n",
      "|18-25|     F|\n",
      "|  55+|     M|\n",
      "|  55+|     F|\n",
      "|36-45|     M|\n",
      "|26-35|     F|\n",
      "| 0-17|     M|\n",
      "|36-45|     F|\n",
      "|51-55|     M|\n",
      "|26-35|     M|\n",
      "|46-50|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##To get the DataFrame without any duplicate rows of given a DataFrame\n",
    "##Use dropDuplicates operation to drop the duplicate rows of a DataFrame. \n",
    "## In this command, performing this on two columns Age and Gender of train dataset and \n",
    "## Get the all unique rows for these two columns.\n",
    "trainDF.select('Age','Gender').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166821\n",
      "166821\n",
      "166821\n"
     ]
    }
   ],
   "source": [
    "## To drop the all rows with null value?\n",
    "## Use dropna operation. \n",
    "## To drop row from the DataFrame it consider three options.\n",
    "## how – ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "## thresh – int, default None If specified, drop rows that have less than thresh non-null values. \n",
    "## This overwrites the how parameter.\n",
    "\n",
    "## subset – optional list of column names to consider.\n",
    "\n",
    "##Drop null rows in train with default parameters and count the rows in output DataFrame. \n",
    "## Default options are any, None, None for how, thresh, subset respectively.\n",
    "print(trainDF.dropna().count())\n",
    "print(trainDF.na.drop().count())\n",
    "print(trainDF.na.drop(\"any\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              -1.0|              -1.0|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              -1.0|              -1.0|    1422|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|              14.0|              -1.0|    1057|\n",
      "|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|              -1.0|              -1.0|    7969|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To replace the null values in DataFrame with constant number\n",
    "## Use fillna operation. \n",
    "\n",
    "##The fillna will take two parameters to fill the null values.\n",
    "## value:\n",
    "##     It will take a dictionary to specify which column will replace with which value.\n",
    "##     A value (int , float, string) for all columns.\n",
    "##subset: Specify some selected columns.\n",
    "\n",
    "##Fill ‘-1’ inplace of null values in train DataFrame.\n",
    "trainDF.fillna(-1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filling with different values for different columns\n",
    "fill_cols_vals = {\n",
    "\"Gender\": 'M',\n",
    "\"Purchase\" : 999999\n",
    "}\n",
    "trainDF.na.fill(fill_cols_vals).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.na.replace([\"\"], [\"UNKNOWN\"], \"Gender\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n"
     ]
    }
   ],
   "source": [
    "## To filter the rows in train dataset which has Purchases more than 15000\n",
    "## apply the filter operation on Purchase column in train DataFrame \n",
    "## to filter out the rows with values more than 15000. \n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(trainDF.Purchase > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(col(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(column(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(expr(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(trainDF[\"Purchase\"] > 15000).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Count|\n",
      "+------+\n",
      "|110523|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "COUNT(*) AS Count\n",
    "FROM trainDFTable\n",
    "WHERE Purchase > 15000\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.filter(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'F')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.filter((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'F')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM trainDFTable WHERE Purchase > 15000 AND Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT Age)|\n",
      "+-------------------+\n",
      "|                  7|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "trainDF.select(countDistinct(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(Age)|\n",
      "+--------------------------+\n",
      "|                         7|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "trainDF.select(approx_count_distinct(\"Age\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First and Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+\n",
      "|first(Product_ID, false)|last(Product_ID, false)|\n",
      "+------------------------+-----------------------+\n",
      "|               P00069042|              P00371644|\n",
      "+------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "trainDF.select(first(\"Product_ID\"), last(\"Product_ID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Purchase)|max(Purchase)|\n",
      "+-------------+-------------+\n",
      "|           12|        23961|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "trainDF.select(min(\"Purchase\"), max(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Purchase)|\n",
      "+-------------+\n",
      "|   5095812742|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "trainDF.select(sum(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Purchase)|\n",
      "+----------------------+\n",
      "|             208520914|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "trainDF.select(sumDistinct(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+-----------------+\n",
      "|(total_purchases / total_transactions)|    avg_purchases|   mean_purchases|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "|                     9263.968712959126|9263.968712959126|9263.968712959126|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "trainDF.select(\n",
    "    count(\"Purchase\").alias(\"total_transactions\"),\n",
    "    sum(\"Purchase\").alias(\"total_purchases\"),\n",
    "    avg(\"Purchase\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Purchase)\").alias(\"mean_purchases\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+---------------------+\n",
      "|  var_pop(Purchase)| var_samp(Purchase)|stddev_pop(Purchase)|stddev_samp(Purchase)|\n",
      "+-------------------+-------------------+--------------------+---------------------+\n",
      "|2.523114008138533E7|2.523118595059777E7|   5023.060827959914|    5023.065393820567|\n",
      "+-------------------+-------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "trainDF.select(var_pop(\"Purchase\"), var_samp(\"Purchase\"),\n",
    "  stddev_pop(\"Purchase\"), stddev_samp(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------------------------+------------------------------------+-------------------------------------+\n",
      "|var_pop(CAST(Purchase AS DOUBLE))|var_samp(CAST(Purchase AS DOUBLE))|stddev_pop(CAST(Purchase AS DOUBLE))|stddev_samp(CAST(Purchase AS DOUBLE))|\n",
      "+---------------------------------+----------------------------------+------------------------------------+-------------------------------------+\n",
      "|              2.523114008138533E7|               2.523118595059777E7|                   5023.060827959914|                    5023.065393820567|\n",
      "+---------------------------------+----------------------------------+------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT var_pop(Purchase), var_samp(Purchase),\n",
    "             stddev_pop(Purchase), stddev_samp(Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|skewness(Purchase)|  kurtosis(Purchase)|\n",
      "+------------------+--------------------+\n",
      "|0.6001383671643401|-0.33838539753600205|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "trainDF.select(skewness(\"Purchase\"), kurtosis(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|skewness(CAST(Purchase AS DOUBLE))|kurtosis(CAST(Purchase AS DOUBLE))|\n",
      "+----------------------------------+----------------------------------+\n",
      "|                0.6001383671643401|              -0.33838539753600205|\n",
      "+----------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT skewness(Purchase), kurtosis(Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+---------------------------------------+\n",
      "|corr(Product_Category_1, Purchase)|covar_samp(Product_Category_1, Purchase)|covar_pop(Product_Category_1, Purchase)|\n",
      "+----------------------------------+----------------------------------------+---------------------------------------+\n",
      "|              -0.34370334591990787|                     -6795.6500072045255|                     -6795.637653004668|\n",
      "+----------------------------------+----------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "trainDF.select(corr(\"Product_Category_1\", \"Purchase\"), covar_samp(\"Product_Category_1\", \"Purchase\"),\n",
    "    covar_pop(\"Product_Category_1\", \"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+\n",
      "|corr(CAST(Product_Category_1 AS DOUBLE), CAST(Purchase AS DOUBLE))|covar_samp(CAST(Product_Category_1 AS DOUBLE), CAST(Purchase AS DOUBLE))|covar_pop(CAST(Product_Category_1 AS DOUBLE), CAST(Purchase AS DOUBLE))|\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+\n",
      "|                                              -0.34370334591990787|                                                     -6795.6500072045255|                                                     -6795.637653004668|\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT corr(Product_Category_1, Purchase), covar_samp(Product_Category_1, Purchase),\n",
    "             covar_pop(Product_Category_1, Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    collect_set(Age)|   collect_list(Age)|\n",
      "+--------------------+--------------------+\n",
      "|[55+, 51-55, 0-17...|[0-17, 0-17, 0-17...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "trainDF.agg(collect_set(\"Age\"), collect_list(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    collect_set(Age)|   collect_list(Age)|\n",
      "+--------------------+--------------------+\n",
      "|[55+, 51-55, 0-17...|[0-17, 0-17, 0-17...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT collect_set(Age), collect_list(Age) FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|  Age|Gender| count|\n",
      "+-----+------+------+\n",
      "|51-55|     F|  9894|\n",
      "|18-25|     M| 75032|\n",
      "| 0-17|     F|  5083|\n",
      "|46-50|     M| 32502|\n",
      "|18-25|     F| 24628|\n",
      "|  55+|     M| 16421|\n",
      "|  55+|     F|  5083|\n",
      "|36-45|     M| 82843|\n",
      "|26-35|     F| 50752|\n",
      "| 0-17|     M| 10019|\n",
      "|36-45|     F| 27170|\n",
      "|51-55|     M| 28607|\n",
      "|26-35|     M|168835|\n",
      "|46-50|     F| 13199|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\", \"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------------+\n",
      "|  Age|  quan|count(Purchase)|\n",
      "+-----+------+---------------+\n",
      "|18-25| 99660|          99660|\n",
      "|26-35|219587|         219587|\n",
      "| 0-17| 15102|          15102|\n",
      "|46-50| 45701|          45701|\n",
      "|51-55| 38501|          38501|\n",
      "|36-45|110013|         110013|\n",
      "|  55+| 21504|          21504|\n",
      "+-----+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\").agg(\n",
    "  count(\"Purchase\").alias(\"quan\"),\n",
    "  expr(\"count(Purchase)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+\n",
      "|  Age|    avg(Purchase)|stddev_pop(Purchase)|\n",
      "+-----+-----------------+--------------------+\n",
      "|18-25|9169.663606261289|   5034.296739627792|\n",
      "|26-35|9252.690632869888|   5010.515894010163|\n",
      "| 0-17|8933.464640444974|   5110.944823427662|\n",
      "|46-50|9208.625697468327|   4967.162022122702|\n",
      "|51-55|9534.808030960236|   5087.302011173864|\n",
      "|36-45|9331.350694917874|   5022.901050378531|\n",
      "|  55+|9336.280459449405|   5011.377469555773|\n",
      "+-----+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\").agg(expr(\"avg(Purchase)\"),expr(\"stddev_pop(Purchase)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|  Age|    avg(Purchase)|\n",
      "+-----+-----------------+\n",
      "|18-25|9169.663606261289|\n",
      "|26-35|9252.690632869888|\n",
      "| 0-17|8933.464640444974|\n",
      "|46-50|9208.625697468327|\n",
      "|51-55|9534.808030960236|\n",
      "|36-45|9331.350694917874|\n",
      "|  55+|9336.280459449405|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To find the mean of each age group in train dataset - Average purchases in each age group\n",
    "trainDF.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|  Age|sum(Purchase)|\n",
      "+-----+-------------+\n",
      "|18-25|    913848675|\n",
      "|26-35|   2031770578|\n",
      "| 0-17|    134913183|\n",
      "|46-50|    420843403|\n",
      "|51-55|    367099644|\n",
      "|36-45|   1026569884|\n",
      "|  55+|    200767375|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupby('Age').agg({'Purchase': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "|  Age|sum(City_Category)|sum(Product_Category_3)|sum(Marital_Status)|sum(Purchase)|sum(User_ID)|sum(Occupation)|sum(Stay_In_Current_City_Years)|sum(Product_Category_1)|sum(Age)|sum(Gender)|sum(Product_Category_2)|sum(Product_ID)|\n",
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "|18-25|              null|               388041.0|              21116|    913848675| 99939196632|         671348|                       116997.0|                 509371|    null|       null|               654936.0|           null|\n",
      "|26-35|              null|               846624.0|              86291|   2031770578|220270500414|        1734073|                       275611.0|                1166945|    null|       null|              1473278.0|           null|\n",
      "| 0-17|              null|                57725.0|                  0|    134913183| 15143112813|         132309|                        20320.0|                  76775|    null|       null|                96155.0|           null|\n",
      "|46-50|              null|               173059.0|              33011|    420843403| 45846804203|         389239|                        51742.0|                 262424|    null|       null|               315572.0|           null|\n",
      "|51-55|              null|               146334.0|              27662|    367099644| 38615925320|         339198|                        44243.0|                 222313|    null|       null|               267570.0|           null|\n",
      "|36-45|              null|               424412.0|              43636|   1026569884|110350311441|         972225|                       148162.0|                 604438|    null|       null|               750081.0|           null|\n",
      "|  55+|              null|                77134.0|              13621|    200767375| 21568218459|         204346|                        26277.0|                 130450|    null|       null|               147356.0|           null|\n",
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Apply sum, min, max, count with groupby to get different summary insight for each group. \n",
    "exprs = {x: \"sum\" for x in trainDF.columns}\n",
    "trainDF.groupBy(\"Age\").agg(exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in Python\n",
    "person = spark.createDataFrame([\n",
    "    (0, \"Dr. Murthy\", 0, [250, 100]),\n",
    "    (1, \"Dr. Sridhar Pappu\", 1, [500, 250, 100]),\n",
    "    (2, \"Dr. Manoj\", 2, [100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"role_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Ph.D\", \"School of Information\", \"Carnegie Mellon University\"),\n",
    "    (1, \"Ph.D\", \"The University of Texas\", \"El Paso\"),\n",
    "    (2, \"Ph.D.\", \"School of Information\", \"Oklahoma State University\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "roleStatus = spark.createDataFrame([\n",
    "    (500, \"President\"),\n",
    "    (250, \"Founder\"),\n",
    "    (100, \"Mentor\")])\\\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+\n",
      "| id|             name|graduate_program|    role_status|\n",
      "+---+-----------------+----------------+---------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2|        Dr. Manoj|               2|          [100]|\n",
      "+---+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------------+--------------------------+\n",
      "|id |degree|department             |school                    |\n",
      "+---+------+-----------------------+--------------------------+\n",
      "|0  |Ph.D  |School of Information  |Carnegie Mellon University|\n",
      "|1  |Ph.D  |The University of Texas|El Paso                   |\n",
      "|2  |Ph.D. |School of Information  |Oklahoma State University |\n",
      "+---+------+-----------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|   status|\n",
      "+---+---------+\n",
      "|500|President|\n",
      "|250|  Founder|\n",
      "|100|   Mentor|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roleStatus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"personTbl\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgramTbl\")\n",
    "roleStatus.createOrReplaceTempView(\"roleStatusTbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl INNER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl FULL OUTER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl LEFT OUTER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Right Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl RIGHT OUTER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Joins\n",
    "Natural joins make implicit guesses at the columns on which you would like to join. \n",
    "It finds matching columns and returns the results. \n",
    "Left, right, and outer natural joins are all supported.\n",
    "\n",
    "WARNING:\n",
    "Implicit is always dangerous! \n",
    "The following query will give us incorrect results because \n",
    "the two DataFrames/tables share a column name (id), but it means different things in the datasets. \n",
    "You should always use this join with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl NATURAL JOIN personTbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross (Cartesian) Joins\n",
    "Cross-joins in simplest terms are inner joins that do not specify a predicate. \n",
    "Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. \n",
    "This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. \n",
    "If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. \n",
    "For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl CROSS JOIN personTbl\n",
    "  ON graduateProgramTbl.id = personTbl.graduate_program\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school| id|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|  2|        Dr. Manoj|               2|          [100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|  2|        Dr. Manoj|               2|          [100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|  2|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+---+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl CROSS JOIN personTbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins on Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|personId|             name|graduate_program|    role_status| id|   status|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
      "|       2|        Dr. Manoj|               2|          [100]|100|   Mentor|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    "  .join(roleStatus, expr(\"array_contains(role_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|personId|             name|graduate_program|    role_status| id|   status|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
      "|       2|        Dr. Manoj|               2|          [100]|100|   Mentor|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM\n",
    "  (select id as personId, name, graduate_program, role_status FROM personTbl)\n",
    "  INNER JOIN roleStatusTbl ON array_contains(role_status, id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110322, 109779)\n"
     ]
    }
   ],
   "source": [
    "## To create a sample DataFrame from the base DataFrame\n",
    "## Use sample operation to take sample of a DataFrame. \n",
    "## The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. \n",
    "## The sample method takes 3 parameters.\n",
    "## withReplacement = True or False to select a observation with or without replacement.\n",
    "## fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame.\n",
    "## seed to reproduce the result\n",
    "sampleDF1 = trainDF.sample(False, 0.2, 1234)\n",
    "sampleDF2 = trainDF.sample(False, 0.2, 4321)\n",
    "print(sampleDF1.count(), sampleDF2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385461\n",
      "164607\n"
     ]
    }
   ],
   "source": [
    "splitDF = trainDF.randomSplit([0.7, 0.3], seed=1234)\n",
    "print(splitDF[0].count())\n",
    "print(splitDF[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000001), 1),\n",
       " (Row(User_ID=1000002), 1)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To apply map operation on DataFrame columns\n",
    "## Apply a function on each row of DataFrame using map operation. \n",
    "## After applying this function, we get the result in the form of RDD. \n",
    "## Apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) \n",
    "## ----- Applying lambda function.\n",
    "\n",
    "trainDF.select('User_ID').rdd.map(lambda x:(x,1)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Prior to Spark 2.0, spark_df.map would alias to spark_df.rdd.map(). \n",
    "With Spark 2.0, you must explicitly call .rdd first.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|              15.0|              null|   23961|\n",
      "|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|              15.0|              null|   23961|\n",
      "|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|              15.0|              null|   23961|\n",
      "|1003045| P00052842|     M|46-50|         1|            B|                         2|             1|                10|              15.0|              null|   23960|\n",
      "|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|              16.0|              null|   23960|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To sort the DataFrame based on column(s)\n",
    "## Use orderBy operation on DataFrame to get sorted output based on some column. \n",
    "## The orderBy operation take two arguments.\n",
    "## List of columns.\n",
    "## ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
    "## Sort the train DataFrame based on ‘Purchase’.\n",
    "trainDF.orderBy(trainDF.Purchase.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition and Coalesce\n",
    "Another important optimization opportunity is to partition the data according to some frequently filtered columns\n",
    "which controls the physical layout of data across the cluster including the partitioning scheme and the number of\n",
    "partitions.\n",
    "\n",
    "Repartition will incur a full shuffle of the data, regardless of whether or not one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of\n",
    "partitions or when you are looking to partition by a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find existing partitions count\n",
    "trainDF.rdd.getNumPartitions()\n",
    "## Do the repartition\n",
    "## trainDF.repartition(5)\n",
    "\n",
    "## Repartition based on a column\n",
    "## If we know we are going to be filtering by a certain column often, \n",
    "## it can be worth repartitioning based on that column.\n",
    "## trainDF.repartition(col(“Purchase”))\n",
    "\n",
    "## We can optionally specify the number of partitions we would like too.\n",
    "## trainDF.repartition(5, col(“Purchase”))\n",
    "\n",
    "## Coalesce on the other hand will not incur a full shuffle and will try to combine partitions. \n",
    "## This operation will shuffle our data into 5 partitions based on the Purchase, \n",
    "## then coalesce them (without a full shuffle).\n",
    "## trainDF.repartition(5, col(\"Purchase\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "DataFrame-1\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "+---+-------+---+\n",
      "\n",
      "None\n",
      "DataFrame-2\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  2|   Ben| 66|\n",
      "|  4|Daniel| 28|\n",
      "|  6| Frank| 64|\n",
      "|  8|Harley| 29|\n",
      "| 10|  Jack| 35|\n",
      "| 12|Litmya| 45|\n",
      "+---+------+---+\n",
      "\n",
      "None\n",
      "After\n",
      "DataFrame-1\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "|  2|    Ben| 66|\n",
      "|  4| Daniel| 28|\n",
      "|  6|  Frank| 64|\n",
      "|  8| Harley| 29|\n",
      "| 10|   Jack| 35|\n",
      "| 12| Litmya| 45|\n",
      "+---+-------+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[1, 'Alex', 25],[3, 'Carol', 53],[5, 'Emily', 25],[7, 'Gabriel', 32],[9, 'Ilma', 35],[11, 'Kim', 45]], ['id', 'name', 'age'])\n",
    "df2 = spark.createDataFrame([[2, 'Ben', 66],[4, 'Daniel', 28],[6, 'Frank', 64],[8, 'Harley', 29],[10, 'Jack', 35],[12, 'Litmya', 45]], ['id', 'name', 'age'])\n",
    "print(\"Before\")\n",
    "print(\"DataFrame-1\")\n",
    "print(df1.show())\n",
    "print(\"DataFrame-2\")\n",
    "print(df2.show())\n",
    "print(\"After\")\n",
    "df1 = df1.union(df2)\n",
    "print(\"DataFrame-1\")\n",
    "print(df1.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions and condtional append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "|  4| Daniel| 28|\n",
      "|  8| Harley| 29|\n",
      "| 10|   Jack| 35|\n",
      "| 12| Litmya| 45|\n",
      "|  4| Daniel| 28|\n",
      "|  8| Harley| 29|\n",
      "| 10|   Jack| 35|\n",
      "| 12| Litmya| 45|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).where(\"age < 60\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Purchase|Purchase_new|\n",
      "+--------+------------+\n",
      "|    8370|      4185.0|\n",
      "|   15200|      7600.0|\n",
      "|    1422|       711.0|\n",
      "|    1057|       528.5|\n",
      "|    7969|      3984.5|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To add the new column in DataFrame\n",
    "## Use withColumn operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. \n",
    "## The withColumn operation will take 2 parameters.\n",
    "## Column name to be added /replaced.\n",
    "## Expression on column.\n",
    "\n",
    "## Derive new column, ‘Purchase_new’ in train which is calculated by dviding Purchase column by 2.\n",
    "\n",
    "trainDF.withColumn('Purchase_new', trainDF.Purchase /2.0).select('Purchase','Purchase_new').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To drop a column in DataFrame\n",
    "## To drop a column from the DataFrame use drop operation. \n",
    "## Drop the column called ‘Comb’ from the test and get the remaining columns in test dataframe\n",
    "testDF.drop('Comb').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To remove some categories of Product_ID column in test that are not present in Product_ID column in train\n",
    "## Use an user defined function ( udf ) to remove the categories of a column which are in test but not in train.\n",
    "## Calculate the categories in Product_ID column which are in test but not in train.\n",
    "diff_cat_in_train_test=testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "diff_cat_in_train_test.count() # For distict count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Product_ID|\n",
      "+----------+\n",
      "| P00074942|\n",
      "| P00030342|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_cat_in_train_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<type 'list'>\n",
      "[u'P00074942', u'P00030342', u'P00279042', u'P00140842', u'P00058842']\n"
     ]
    }
   ],
   "source": [
    "## There are 46 different categories in test. \n",
    "## To remove these categories from the test ‘Product_ID’ column.\n",
    "\n",
    "## Create the distinct list of categories called ‘not_found_cat’ from the diff_cat_in_train_test using map operation.\n",
    "## Register a udf(user define function).\n",
    "## User defined function will take each element of test column and search this in not_found_cat list and \n",
    "## it will put -1 ifit finds in this list otherwise it will do nothing.\n",
    "not_found_cat = diff_cat_in_train_test.rdd.map(lambda x: x[0]).collect()\n",
    "print(len(not_found_cat))\n",
    "print(type(not_found_cat))\n",
    "print(not_found_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined Functions - UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Register the udf, we need to import StringType from the pyspark.sql and udf from the pyspark.sql.functions. \n",
    "## The udf function takes 2 parameters as arguments:\n",
    "## Return type (in my case StringType())\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "Function1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|NEW_Product_ID|\n",
      "+--------------+\n",
      "|            -1|\n",
      "|            -1|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In the above code function name is ‘Function1’ and we are putting ‘-1’  for not found catagories in test ‘Product_ID’. \n",
    "## Finally apply above ‘Function1’ function on test ‘Product_ID’ and take result in k for new column calles “NEW_Product_ID”.\n",
    "\n",
    "k = testDF.withColumn(\"NEW_Product_ID\",Function1(testDF[\"Product_ID\"])).select('NEW_Product_ID')\n",
    "k.where(k['NEW_Product_ID'] == -1).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## See the results by again calculating the different categories in k and train subtract operation.\n",
    "diff_cat_in_train_test=k.select('NEW_Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "print(diff_cat_in_train_test.count())# For distinct count\n",
    "print(diff_cat_in_train_test.distinct().count())# For distinct count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(NEW_Product_ID=u'-1')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The output 1 means we have now only 1 different category k and train.\n",
    "diff_cat_in_train_test.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "trainDF.select(round(lit(\"2.5\")), bround(lit(2.5))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|            3|             2|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT round(2.5), bround(2.5)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.34370334592\n",
      "+----------------------------------+\n",
      "|corr(Purchase, Product_Category_1)|\n",
      "+----------------------------------+\n",
      "|              -0.34370334591990787|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "print(trainDF.stat.corr(\"Purchase\", \"Product_Category_1\"))\n",
    "trainDF.select(corr(\"Purchase\", \"Product_Category_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|corr(CAST(Purchase AS DOUBLE), CAST(Product_Category_1 AS DOUBLE))|\n",
      "+------------------------------------------------------------------+\n",
      "|                                              -0.34370334591990787|\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT corr(Purchase, Product_Category_1) FROM trainDFTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|Age_freqItems                                 |\n",
      "+----------------------------------------------+\n",
      "|[18-25, 26-35, 55+, 46-50, 51-55, 36-45, 0-17]|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.stat.freqItems([\"Age\"]).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "trainDF.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "|ltrim( HELLLOOOO )|rtrim( HELLLOOOO )|trim( HELLLOOOO )|lpad(HELLOOOO , 3,  )|rpad(HELLOOOO , 10,  )|\n",
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "ltrim(' HELLLOOOO '),\n",
    "rtrim(' HELLLOOOO '),\n",
    "trim(' HELLLOOOO '),\n",
    "lpad('HELLOOOO ', 3, ' '),\n",
    "rpad('HELLOOOO ', 10, ' ')\n",
    "FROM\n",
    "trainDFTable\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"F|M\"\n",
    "\n",
    "trainDF.select(\n",
    "regexp_replace(col(\"Gender\"), regex_string, \"MALE_OR_FEMALE\")\n",
    ".alias(\"Gender_DECODE\"),\n",
    "col(\"Gender\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "regexp_replace(Gender, 'F|M', 'MALE_OR_FEMALE') as\n",
    "Gender_DECODE,\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|translate(Gender, FM, 01)|Gender|\n",
      "+-------------------------+------+\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "+-------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "trainDF.select(\n",
    "translate(col(\"Gender\"), \"FM\", \"01\"),\n",
    "col(\"Gender\"))\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|translate(Gender, FM, 01)|Gender|\n",
      "+-------------------------+------+\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "+-------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "translate(Gender, 'FM', '01'),\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|1  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|2  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|3  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|4  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|5  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|6  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|7  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|8  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "|9  |2018-11-20|2018-11-20 23:40:37.835|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"dateDFTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2018-11-15|        2018-11-25|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5),date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2018-11-15|        2018-11-25|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "date_sub(today, 5),\n",
    "date_add(today, 5)\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF\\\n",
    ".withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|               -13.5483871|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF\\\n",
    ".select(\n",
    "to_date(lit(\"2017-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2018-02-18\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "|to_date('2016-01-01')|months_between(CAST(2016-01-01 AS TIMESTAMP), CAST(2017-01-01 AS TIMESTAMP))|datediff(CAST(2016-01-01 AS DATE), CAST(2017-01-01 AS DATE))|\n",
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "|           2016-01-01|                                                                       -12.0|                                                        -366|\n",
      "|           2016-01-01|                                                                       -12.0|                                                        -366|\n",
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date('2016-01-01'),\n",
    "months_between('2016-01-01', '2017-01-01'),\n",
    "datediff('2016-01-01', '2017-01-01')\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    ".select(to_date(col(\"date\")))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING__\n",
    "<br>Spark will not throw an error if it cannot parse the date, it’ll just return null. This can be a bit tricky in larger pipelines because you may be expecting your data in one format and getting it in another. To illustrate, let’s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 2016-20-12 - year-day-month\n",
    "### 2017-12-11 - year-month-day\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "\n",
    "cleanDateDF = spark.range(1)\\\n",
    ".select(to_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date\"),\n",
    "to_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "|to_date(CAST(unix_timestamp(datetable2.`date`, 'yyyy-dd-MM') AS TIMESTAMP))|to_date(CAST(unix_timestamp(datetable2.`date2`, 'yyyy-dd-MM') AS TIMESTAMP))|to_date(datetable2.`date`)|\n",
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "|                                                                 2017-11-12|                                                                  2017-12-20|                2017-11-12|\n",
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date(cast(unix_timestamp(date, 'yyyy-dd-MM') as timestamp)),\n",
    "to_date(cast(unix_timestamp(date2, 'yyyy-dd-MM') as timestamp)),\n",
    "to_date(date)\n",
    "FROM\n",
    "dateTable2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|CAST(unix_timestamp(date, yyyy-dd-MM) AS TIMESTAMP)|\n",
      "+---------------------------------------------------+\n",
      "|2017-11-12 00:00:00                                |\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF\\\n",
    ".select(unix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|        Description|\n",
      "+---+-------------------+\n",
      "|  0|This is long string|\n",
      "|  1|This is long string|\n",
      "|  2|This is long string|\n",
      "|  3|This is long string|\n",
      "|  4|This is long string|\n",
      "|  5|This is long string|\n",
      "|  6|This is long string|\n",
      "|  7|This is long string|\n",
      "|  8|This is long string|\n",
      "|  9|This is long string|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textDF = spark.range(10).withColumn(\"Description\", lit(\"This is long string\"))\n",
    "textDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|split(Description,  )|\n",
      "+---------------------+\n",
      "| [This, is, long, ...|\n",
      "| [This, is, long, ...|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "textDF.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|split(Description,  )|\n",
      "+---------------------+\n",
      "| [This, is, long, ...|\n",
      "| [This, is, long, ...|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textDF.createOrReplaceTempView('textDFTable')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "split(Description, ' ')\n",
    "FROM\n",
    "textDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function is created, we need to register them with Spark so that we can used\n",
    "them on all of our worker machines. Spark will serialize the function on the driver, and transfer it over the network to all executor processes. This happens regardless of language.\n",
    "\n",
    "<br>Once we go to use the function, there are essentially two different things that occur. If the function is written in Scala or Java then we can use that function within the JVM. This means there will be little performance penalty aside from the fact that we can’t take advantage of code generation capabilities that Spark has for built-in functions.\n",
    "\n",
    "<br>If the function is written in Python, something quite different happens. \n",
    "Spark will start up a python process on the worker, serialize all of the data to a format that python can understand (remember it was in the JVM before), execute the function row by row on that data in the python process, before finally returning the results of the row operations to the JVM and Spark.\n",
    "\n",
    "![UDF_Spark_Python](./Images/UDF_Spark_Python.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Different Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are variety of data sources that one can use out of the box aswell as the countless other sources built by the greater community.\n",
    "\n",
    "<br> **Spark** has six “core” data sources and hundreds of external data sources written by the community.\n",
    "\n",
    "-  CSV\n",
    "-  JSON\n",
    "-  Parquet\n",
    "-  ORC\n",
    "-  JDBC/ODBC Connections\n",
    "-  Plain-text files\n",
    "\n",
    "<br> As mentioned, Spark has numerous community-created data sources. Here’s just a small sample:\n",
    "-  Cassandra\n",
    "-  HBase\n",
    "-  MongoDB\n",
    "-  AWS Redshift\n",
    "-  XML\n",
    "-  And many many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read API Structure**\n",
    "<br>DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load(...)\n",
    "<br>After we have a DataFrame reader, we specify several values:\n",
    "-  The format\n",
    "-  The schema\n",
    "-  The read mode\n",
    "-  A series of options\n",
    "\n",
    "*Ex. spark.read.format(\"csv\")\n",
    "<br>  .option(\"mode\", \"FAILFAST\")\n",
    "<br>  .option(\"inferSchema\", \"true\")\n",
    "<br>  .option(\"path\", \"path/to/file(s)\")\n",
    "<br>  .schema(someSchema)\n",
    "<br>  .load()\n",
    "*\n",
    "\n",
    "** READ MODES **\n",
    "-  permissive - Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record.\n",
    "-  dropMalformed - Drops the row that contains malformed records\n",
    "-  failFast - Fails immediately upon encountering malformed records\n",
    "<br><br>The default is permissive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write API Structure **\n",
    "<br>We will use this format to write to all of our data sources. \n",
    "<br>format is optional because by default, Spark will use the **Parquet** format. \n",
    "<br>option, again, allows us to configure how to write out our given data. \n",
    "<br>PartitionBy, bucketBy, and sortBy work only for file-based data sources; \n",
    "<br>you can use them to control the specific layout of files at the destination.\n",
    "\n",
    "<br> DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n",
    "<br> The foundation for writing data is quite similar to that of reading data. \n",
    "<br>Instead of the DataFrameReader, we have the DataFrameWriter. \n",
    "<br>Because we always need to write out some given data source, \n",
    "<br>we access the DataFrameWriter on a per-DataFrame basis via the write attribute:\n",
    "\n",
    "<br>After we have a DataFrameWriter, we specify three values: the format, a series of options, and the save mode. \n",
    "\n",
    "<br>Example: \n",
    "<br>dataframe.write.format(\"csv\")\n",
    "<br>  .option(\"mode\", \"OVERWRITE\")\n",
    "<br>  .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "<br>  .option(\"path\", \"path/to/file(s)\")\n",
    "<br>  .save()\n",
    "\n",
    "** SAVE MODES **\n",
    "-  append - Appends the output files to the list of files that already exist at that location\n",
    "-  overwrite - Will completely overwrite any data that already exists there\n",
    "-  errorIfExists - Throws an error and fails the write if data or files already exist at the specified location\n",
    "-  ignore - If data or files exist at the location, do nothing with the current DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcs_CSV_DF = spark.read.format(\"csv\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(\"file:///home/insofe/IBM/Day03/TCS_NSE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+---------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close|Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+---------+--------+\n",
      "|2004-08-25 00:00:00|149.837006|149.837006|122.375000|123.494003|67.378128|  136928|\n",
      "|2004-08-26 00:00:00|124.000000|124.625000|121.912003|122.375000|66.767601|40443200|\n",
      "|2004-08-27 00:00:00|122.800003|122.800003|119.820000|120.332001|65.652962|30646000|\n",
      "|2004-08-30 00:00:00|121.237999|123.750000|120.625000|123.345001|67.296806|24465208|\n",
      "+-------------------+----------+----------+----------+----------+---------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_CSV_DF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_CSV_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "tcs_CSV_DF = tcs_CSV_DF.select(col(\"Date\").cast(\"date\"), \n",
    "                     col(\"Open\").cast(\"double\"),\n",
    "                     col(\"High\").cast(\"double\"),\n",
    "                     col(\"Low\").cast(\"double\"),\n",
    "                     col(\"Close\").cast(\"double\"),\n",
    "                     col(\"Adj Close\").cast(\"double\"), \n",
    "                     col(\"Volume\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_CSV_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcs_CSV_DF = tcs_CSV_DF.withColumnRenamed(\"Adj Close\", \"Adj_Close\")\n",
    "tcs_CSV_DF = tcs_CSV_DF.withColumnRenamed(\"Date\", \"Stock_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcs_CSV_DF.write.format(\"json\").mode(\"overwrite\").save(\"file:///home/insofe/IBM/Day03/TCS_JSON/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Files\n",
    "Those coming from the world of JavaScript are likely familiar with JavaScript Object Notation, or JSON, as it’s commonly called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcs_JSON_DF = spark.read.format(\"json\")\\\n",
    ".option(\"inferSchema\", \"True\")\\\n",
    ".load(\"file:///home/insofe/IBM/Day03/TCS_JSON/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+--------+\n",
      "|Adj_Close|     Close|      High|       Low|      Open|Stock_Date|  Volume|\n",
      "+---------+----------+----------+----------+----------+----------+--------+\n",
      "|67.378128|123.494003|149.837006|   122.375|149.837006|2004-08-25|  136928|\n",
      "|66.767601|   122.375|   124.625|121.912003|     124.0|2004-08-26|40443200|\n",
      "|65.652962|120.332001|122.800003|    119.82|122.800003|2004-08-27|30646000|\n",
      "|67.296806|123.345001|    123.75|   120.625|121.237999|2004-08-30|24465208|\n",
      "+---------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_JSON_DF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Files\n",
    "Spark also allows you to read in plain-text files. \n",
    "<br>Each line in the file becomes a record in the DataFrame. \n",
    "<br>It is then up to you to transform it accordingly. \n",
    "<br>As an example of how you would do this, \n",
    "<br>suppose that you need to parse some Apache log files to some more structured format, \n",
    "<br>or perhaps you want to parse some plain text for natural-language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcs_TEXT_DF = spark.read.text(\"file:///home/insofe/IBM/Day03/TCS_NSE.csv\")\\\n",
    "  .selectExpr(\"split(value, ',') as rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[Date, Open, High...|\n",
      "|[2004-08-25, 149....|\n",
      "|[2004-08-26, 124....|\n",
      "|[2004-08-27, 122....|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_TEXT_DF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Databases\n",
    "<br>driver = \"org.sqlite.JDBC\"\n",
    "<br>url = \"jdbc:sqlite:\" + path\n",
    "<br>tablename = \"flight_info\"\n",
    "\n",
    "<br>dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    "<br>  .option(\"dbtable\", tablename).option(\"driver\",  driver).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pgDF = spark.read.format(\"jdbc\")\\\n",
    "<br>  .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "<br>  .option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    "<br>  .option(\"dbtable\", \"schema.tablename\")\\\n",
    "<br>  .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Query Pushdown **\n",
    "<br> Spark makes a best-effort attempt to filter data in the database itself before creating the DataFrame. \n",
    "<br> Only the intended columns are selected..\n",
    "\n",
    "<br>Ex. dbDataFrame.select(\"Age\").distinct()\n",
    "<br> Above query will fetch only the \"Age\" column from the original database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can actually do better than this on certain queries. \n",
    "<br>For example, if we specify a filter on our DataFrame, Spark will push that filter down into the database. \n",
    "<br>Ex. dbDataFrame.filter(\"Age > 50\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HIVE Table for the previously created DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tcs_CSV_DF.write.mode(\"overwrite\").saveAsTable(\"tcs_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "|Stock_Date|      Open|      High|       Low|     Close|Adj_Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "|2004-08-25|149.837006|149.837006|   122.375|123.494003|67.378128|  136928|\n",
      "|2004-08-26|     124.0|   124.625|121.912003|   122.375|66.767601|40443200|\n",
      "|2004-08-27|122.800003|122.800003|    119.82|120.332001|65.652962|30646000|\n",
      "|2004-08-30|121.237999|    123.75|   120.625|123.345001|67.296806|24465208|\n",
      "|2004-08-31|123.311996|    123.75|     122.0|123.512001|67.387932|21194656|\n",
      "|2004-09-01|    123.75|   124.375|122.949997|123.487999|67.374817|19935544|\n",
      "|2004-09-02|123.737999|125.574997|    123.25|124.207001|67.767113|21356352|\n",
      "|2004-09-03|    125.75|     137.5|123.794998|124.732002|68.053581| 9869856|\n",
      "|2004-09-06|129.988007|129.988007|124.112999|124.357002|67.848961| 9038672|\n",
      "|2004-09-07|   129.375|   129.375|   124.375|124.449997|67.899704| 5772232|\n",
      "+----------+----------+----------+----------+----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM tcs_stocks LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------+\n",
      "|database|tableName   |isTemporary|\n",
      "+--------+------------+-----------+\n",
      "|        |traindftable|true       |\n",
      "+--------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES LIKE 'trainDF*'\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database|tableName |isTemporary|\n",
      "+--------+----------+-----------+\n",
      "|default |tcs_stocks|false      |\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES LIKE 'tcs*'\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------------+------------------+------------------+------------------+----------+\n",
      "|year|week|          avg_open|          avg_high|           avg_low|         avg_close|avg_volume|\n",
      "+----+----+------------------+------------------+------------------+------------------+----------+\n",
      "|2018|  46|1904.1666666666667|1928.3000083333334|1893.9166666666667|1912.1333009999998| 1933389.0|\n",
      "|2018|  45|1921.6666666666667|1942.2666423333333|1893.5833333333333|1910.9166260000002| 1894185.0|\n",
      "|2018|  44|1890.7099852000003|1926.0399901999997|1862.2199951999999|      1910.6300048| 2474390.0|\n",
      "|2018|  43|      1870.3900146|      1883.7600096|      1830.2399904|           1849.45| 2412084.6|\n",
      "|2018|  42|           1949.25|     1961.36251825|1911.2625122499999|1938.6000060000001|2915389.75|\n",
      "|2018|  41|            2049.2|      2065.3599608|      1994.8699706|2022.2400148000002| 4140080.4|\n",
      "|2018|  40|2161.9500122500003|     2201.89996325|      2105.8000795|2146.4625244999997|3963643.25|\n",
      "|2018|  39|      2165.3599608|      2204.2199706|2136.3400392000003|2179.2699706000003| 2817625.6|\n",
      "|2018|  38|2072.9749755000003|2091.6500244999997|     2057.28753675|      2081.4874265| 2075040.5|\n",
      "|2018|  37|         2067.4375|     2080.71246325|2043.6499937500002|     2059.08746325|2789995.75|\n",
      "+----+----+------------------+------------------+------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT \n",
    "YEAR(stock_date) AS year, \n",
    "WEEKOFYEAR(stock_date) AS week, \n",
    "Avg(open) AS avg_open, \n",
    "Avg(high) AS avg_high, \n",
    "Avg(low) AS avg_low, \n",
    "Avg(close) AS avg_close, \n",
    "Avg(volume) AS avg_volume \n",
    "FROM tcs_stocks \n",
    "GROUP BY YEAR(stock_date), WEEKOFYEAR(stock_date) \n",
    "ORDER BY year DESC, week DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
