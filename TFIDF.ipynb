{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Text Data for Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data requires special preparation before you can start using it for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text must be parsed to remove words, called tokenization. \n",
    "<br>Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of your text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, you will discover exactly how you can prepare your text data for predictive modeling in Python with scikit-learn.\n",
    "- How to convert text to word count vectors with CountVectorizer.\n",
    "- How to convert text to word frequency vectors with TfidfVectorizer.\n",
    "- How to convert text to unique integers with HashingVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We cannot work with text directly when using machine learning algorithms.\n",
    "\n",
    "<br>Instead, we need to convert the text to numbers.\n",
    "\n",
    "<br>We may want to perform classification of documents, so each document is an “input” and a class label is the “output” for our predictive algorithm. Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.\n",
    "\n",
    "<br>A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW.\n",
    "\n",
    "<br>The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.\n",
    "\n",
    "<br>This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.\n",
    "\n",
    "<br>This is the bag of words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order.\n",
    "\n",
    "<br>There are many ways to extend this simple method, both by better clarifying what a “word” is and in defining what to encode about each word in the vector.\n",
    "\n",
    "<br>The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts with CountVectorizer\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "\n",
    "You can use it as follows:\n",
    "\n",
    "1. Create an instance of the CountVectorizer class.\n",
    "2. Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "3. Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "\n",
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n",
    "\n",
    "Because these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package.\n",
    "\n",
    "The vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and better understand what is going on by calling the toarray() function.\n",
    "\n",
    "Below is an example of using the CountVectorizer to tokenize, build a vocabulary, and then encode a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'post': 5, 'graduate': 3, 'program': 6, 'in': 4, 'big': 1, 'data': 2, 'analytics': 0}\n",
      "(1, 7)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 3 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"Post Graduate Program in Big Big Big Data Analytics...!\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that all words were made lowercase by default and that the punctuation was ignored. \n",
    "<br>Refer all of the options in the API documentation at\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the same vectorizer can be used on documents that contain words not included in the vocabulary. \n",
    "<br>These words are ignored and no count is given in the resulting vector.\n",
    "\n",
    "For example, below is an example of using the vectorizer above to encode a document with few words in the vocabulary and few words that are not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'post': 5, 'graduate': 3, 'program': 6, 'in': 4, 'big': 1, 'data': 2, 'analytics': 0}\n",
      "(1, 7)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# encode another document\n",
    "text2 = [\"Big Data analytics training and certification program\"]\n",
    "vector2 = vectorizer.transform(text2)\n",
    "# summarize encoded vector\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector2.shape)\n",
    "print(type(vector2))\n",
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies with TfidfVectorizer\n",
    "Word counts are a good starting point, but are very basic.\n",
    "\n",
    "One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "- Term Frequency: This summarizes how often a given word appears within a document.\n",
    "- Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "\n",
    "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "\n",
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. \n",
    "Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n",
    "\n",
    "The same create, fit, and transform process is used as with the CountVectorizer.\n",
    "\n",
    "Below is an example of using the TfidfVectorizer to learn vocabulary and inverse document frequencies across 4 small documents and then encode one of those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time flies like an arrow',\n",
       " 'Fruit flies like a banana',\n",
       " 'Cat sat on the mat',\n",
       " 'The cat is white.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of text documents\n",
    "data = '''Time flies like an arrow\n",
    "Fruit flies like a banana\n",
    "Cat sat on the mat\n",
    "The cat is white.'''\n",
    "\n",
    "dataset = data.split('\\n')\n",
    "# print dataset contents\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.004s.\n",
      "{'time': 24, 'flies': 7, 'like': 13, 'an': 0, 'arrow': 2, 'time flies': 25, 'flies like': 8, 'like an': 14, 'an arrow': 1, 'fruit': 9, 'banana': 3, 'fruit flies': 10, 'like banana': 15, 'cat': 4, 'sat': 19, 'on': 17, 'the': 21, 'mat': 16, 'cat sat': 6, 'sat on': 20, 'on the': 18, 'the mat': 23, 'is': 11, 'white': 26, 'the cat': 22, 'cat is': 5, 'is white': 12}\n",
      "[1.91629073 1.91629073 1.91629073 1.91629073 1.51082562 1.91629073\n",
      " 1.91629073 1.51082562 1.51082562 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.51082562 1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.51082562 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the transform\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "#max_df=0.95, min_df=2, stop_words='english' #USE HELP TO SEE WHAT EACH DOES)\n",
    "\n",
    "# tokenize and build vocab\n",
    "t0 = time()\n",
    "\n",
    "# tokenize and build vocab\n",
    "tfidf_vectorizer.fit(dataset)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# summarize\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "print(tfidf_vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary of 27 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n",
    "\n",
    "The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.51082562 to the most frequently observed words: “the” at index 4, 7, 8, 13, 21.\n",
    "For the words cat, flies, flies like, like and the."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35657982 0.35657982 0.35657982 0.28113163 0.28113163 0.28113163\n",
      " 0.35657982 0.35657982 0.35657982 0.41292788 0.32555709 0.41292788\n",
      " 0.41292788 0.32555709 0.32555709 0.41292788 0.34829919 0.27460308\n",
      " 0.34829919 0.34829919 0.34829919 0.34829919 0.34829919 0.34829919\n",
      " 0.27460308 0.40021825 0.40021825 0.31553666 0.40021825 0.40021825\n",
      " 0.40021825 0.31553666]\n",
      "Type:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape:  (4, 27)\n",
      "[[0.35657982 0.35657982 0.35657982 0.         0.         0.\n",
      "  0.         0.28113163 0.28113163 0.         0.         0.\n",
      "  0.         0.28113163 0.35657982 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35657982 0.35657982 0.        ]\n",
      " [0.         0.         0.         0.41292788 0.         0.\n",
      "  0.         0.32555709 0.32555709 0.41292788 0.41292788 0.\n",
      "  0.         0.32555709 0.         0.41292788 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.27460308 0.\n",
      "  0.34829919 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.34829919 0.34829919\n",
      "  0.34829919 0.34829919 0.34829919 0.27460308 0.         0.34829919\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.31553666 0.40021825\n",
      "  0.         0.         0.         0.         0.         0.40021825\n",
      "  0.40021825 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31553666 0.40021825 0.\n",
      "  0.         0.         0.40021825]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "tfidf = tfidf_vectorizer.transform(dataset)\n",
    "print(tfidf.data)\n",
    "print(\"Type: \", type(tfidf))\n",
    "print(\"Shape: \", tfidf.shape)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the 4 documents are encoded as an 27-element sparse array and we can review the final scorings of each word with different values for the words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are normalized to values between 0 and 1 and the encoded document vectors can then be used directly with most machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write tfidf as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 27)\n",
      "[[0.35657982 0.35657982 0.35657982 0.         0.         0.\n",
      "  0.         0.28113163 0.28113163 0.         0.         0.\n",
      "  0.         0.28113163 0.35657982 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35657982 0.35657982 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "dense = tfidf.todense()\n",
    "print(dense.shape)\n",
    "print(dense[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "['an', 'an arrow', 'arrow', 'banana', 'cat', 'cat is', 'cat sat', 'flies', 'flies like', 'fruit', 'fruit flies', 'is', 'is white', 'like', 'like an', 'like banana', 'mat', 'on', 'on the', 'sat', 'sat on', 'the', 'the cat', 'the mat', 'time', 'time flies', 'white']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(len(feature_names))\n",
    "print(feature_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>an arrow</th>\n",
       "      <th>arrow</th>\n",
       "      <th>banana</th>\n",
       "      <th>cat</th>\n",
       "      <th>cat is</th>\n",
       "      <th>cat sat</th>\n",
       "      <th>flies</th>\n",
       "      <th>flies like</th>\n",
       "      <th>fruit</th>\n",
       "      <th>...</th>\n",
       "      <th>on the</th>\n",
       "      <th>sat</th>\n",
       "      <th>sat on</th>\n",
       "      <th>the</th>\n",
       "      <th>the cat</th>\n",
       "      <th>the mat</th>\n",
       "      <th>time</th>\n",
       "      <th>time flies</th>\n",
       "      <th>white</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0.35658</td>\n",
       "      <td>0.35658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281132</td>\n",
       "      <td>0.281132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.35658</td>\n",
       "      <td>0.35658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Time flies like an arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruit flies like a banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0.274603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cat sat on the mat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315537</td>\n",
       "      <td>0.400218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315537</td>\n",
       "      <td>0.400218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.400218</td>\n",
       "      <td>The cat is white.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        an  an arrow    arrow    banana       cat    cat is   cat sat  \\\n",
       "0  0.35658   0.35658  0.35658  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.00000   0.00000  0.00000  0.412928  0.000000  0.000000  0.000000   \n",
       "2  0.00000   0.00000  0.00000  0.000000  0.274603  0.000000  0.348299   \n",
       "3  0.00000   0.00000  0.00000  0.000000  0.315537  0.400218  0.000000   \n",
       "\n",
       "      flies  flies like     fruit            ...                on the  \\\n",
       "0  0.281132    0.281132  0.000000            ...              0.000000   \n",
       "1  0.325557    0.325557  0.412928            ...              0.000000   \n",
       "2  0.000000    0.000000  0.000000            ...              0.348299   \n",
       "3  0.000000    0.000000  0.000000            ...              0.000000   \n",
       "\n",
       "        sat    sat on       the   the cat   the mat     time  time flies  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.35658     0.35658   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000     0.00000   \n",
       "2  0.348299  0.348299  0.274603  0.000000  0.348299  0.00000     0.00000   \n",
       "3  0.000000  0.000000  0.315537  0.400218  0.000000  0.00000     0.00000   \n",
       "\n",
       "      white                       text  \n",
       "0  0.000000   Time flies like an arrow  \n",
       "1  0.000000  Fruit flies like a banana  \n",
       "2  0.000000         Cat sat on the mat  \n",
       "3  0.400218          The cat is white.  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DF = pd.DataFrame(dense)\n",
    "DF.columns = tfidf_vectorizer.get_feature_names()\n",
    "DF['text'] = dataset\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an arrow</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banana</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274603</td>\n",
       "      <td>0.315537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat is</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat sat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flies</th>\n",
       "      <td>0.281132</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flies like</th>\n",
       "      <td>0.281132</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fruit</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fruit flies</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is white</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.281132</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like an</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like banana</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sat on</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274603</td>\n",
       "      <td>0.315537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the mat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time flies</th>\n",
       "      <td>0.35658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Time flies like an arrow</td>\n",
       "      <td>Fruit flies like a banana</td>\n",
       "      <td>Cat sat on the mat</td>\n",
       "      <td>The cat is white.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0                          1  \\\n",
       "an                            0.35658                          0   \n",
       "an arrow                      0.35658                          0   \n",
       "arrow                         0.35658                          0   \n",
       "banana                              0                   0.412928   \n",
       "cat                                 0                          0   \n",
       "cat is                              0                          0   \n",
       "cat sat                             0                          0   \n",
       "flies                        0.281132                   0.325557   \n",
       "flies like                   0.281132                   0.325557   \n",
       "fruit                               0                   0.412928   \n",
       "fruit flies                         0                   0.412928   \n",
       "is                                  0                          0   \n",
       "is white                            0                          0   \n",
       "like                         0.281132                   0.325557   \n",
       "like an                       0.35658                          0   \n",
       "like banana                         0                   0.412928   \n",
       "mat                                 0                          0   \n",
       "on                                  0                          0   \n",
       "on the                              0                          0   \n",
       "sat                                 0                          0   \n",
       "sat on                              0                          0   \n",
       "the                                 0                          0   \n",
       "the cat                             0                          0   \n",
       "the mat                             0                          0   \n",
       "time                          0.35658                          0   \n",
       "time flies                    0.35658                          0   \n",
       "white                               0                          0   \n",
       "text         Time flies like an arrow  Fruit flies like a banana   \n",
       "\n",
       "                              2                  3  \n",
       "an                            0                  0  \n",
       "an arrow                      0                  0  \n",
       "arrow                         0                  0  \n",
       "banana                        0                  0  \n",
       "cat                    0.274603           0.315537  \n",
       "cat is                        0           0.400218  \n",
       "cat sat                0.348299                  0  \n",
       "flies                         0                  0  \n",
       "flies like                    0                  0  \n",
       "fruit                         0                  0  \n",
       "fruit flies                   0                  0  \n",
       "is                            0           0.400218  \n",
       "is white                      0           0.400218  \n",
       "like                          0                  0  \n",
       "like an                       0                  0  \n",
       "like banana                   0                  0  \n",
       "mat                    0.348299                  0  \n",
       "on                     0.348299                  0  \n",
       "on the                 0.348299                  0  \n",
       "sat                    0.348299                  0  \n",
       "sat on                 0.348299                  0  \n",
       "the                    0.274603           0.315537  \n",
       "the cat                       0           0.400218  \n",
       "the mat                0.348299                  0  \n",
       "time                          0                  0  \n",
       "time flies                    0                  0  \n",
       "white                         0           0.400218  \n",
       "text         Cat sat on the mat  The cat is white.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write as CSV file\n",
    "DF.to_csv('mytfidf.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc Similarity\n",
    "\n",
    "Given a new query, how to find out which document is it closest to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = 'Time flies like Sam'\n",
    "response = tfidf_vectorizer.transform([new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_array = response.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>an arrow</th>\n",
       "      <th>arrow</th>\n",
       "      <th>banana</th>\n",
       "      <th>cat</th>\n",
       "      <th>cat is</th>\n",
       "      <th>cat sat</th>\n",
       "      <th>flies</th>\n",
       "      <th>flies like</th>\n",
       "      <th>fruit</th>\n",
       "      <th>...</th>\n",
       "      <th>on</th>\n",
       "      <th>on the</th>\n",
       "      <th>sat</th>\n",
       "      <th>sat on</th>\n",
       "      <th>the</th>\n",
       "      <th>the cat</th>\n",
       "      <th>the mat</th>\n",
       "      <th>time</th>\n",
       "      <th>time flies</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401043</td>\n",
       "      <td>0.401043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508672</td>\n",
       "      <td>0.508672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    an  an arrow  arrow  banana  cat  cat is  cat sat     flies  flies like  \\\n",
       "0  0.0       0.0    0.0     0.0  0.0     0.0      0.0  0.401043    0.401043   \n",
       "\n",
       "   fruit  ...     on  on the  sat  sat on  the  the cat  the mat      time  \\\n",
       "0    0.0  ...    0.0     0.0  0.0     0.0  0.0      0.0      0.0  0.508672   \n",
       "\n",
       "   time flies  white  \n",
       "0    0.508672    0.0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(response_array, columns=DF.columns[0:27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.70100165]]), array([[0.39168692]]), array([[0.]]), array([[0.]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "list(map(lambda x: cosine_similarity(response, x), dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Application of TF-IDF in clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "with(open('ende_cleaned.json', 'r')) as f:\n",
    "    corpus = f.readlines()\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!type corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the labels for the documents\n",
    "label1=[0]*90\n",
    "label2=[1]*90\n",
    "labels=label1+label2 #label1 correspond to german and label2 correspond to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features...\n",
      "done in 3.358s.\n"
     ]
    }
   ],
   "source": [
    "#Extractig the features\n",
    "print(\"Extracting tf-idf features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)\n",
    "#max_df=0.95, min_df=2, stop_words='english' #USE HELP TO SEE WHAT EACH DOES)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a sparse matrix \n",
    "dense = tfidf.todense()\n",
    "dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2, init = 'k-means++')\n",
    "model.fit(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 1), (0, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 1), (0, 1), (1, 1), (1, 1), (1, 1), (0, 1), (0, 1), (0, 1), (0, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "kmeans_labels = model.labels_\n",
    "x=list(zip(kmeans_labels,labels))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"url\": \"https://de.wikipedia.org/wiki/Wikipedia:Impressum\", \"article\": \"\\\\u201eWikipedia, Die freie Enzyklop\\\\u00e4die\\\\u201c ist im Internet unter www.wikipedia.org zu finden, die deutschsprachige Ausgabe unter de.wikipedia.org.Anbieterin dieser Website ist die Wikimedia Foundation Inc., eingetragen beim Florida Department of State, Division of Corporations unter der Nummer N03000005323. Die Wikimedia Foundation ist eine Stiftung nach dem Recht des US-Bundesstaates Florida. Die verantwortliche Ansprechperson \\\\u2013 gleichzeitig Designated Agent im Sinne des Digital Millennium Copyright Act \\\\u2013 ist Geoff Brigham.Bei Fragen und Intervieww\\\\u00fcnschen k\\\\u00f6nnen Sie sich auf informeller Basis auch gerne an die aktiven deutschsprachigen Ansprechpartner wenden, siehe Wikipedia:Kontakt und Wikipedia:Presse.Wikipedia ist eine freie Enzyklop\\\\u00e4die. N\\\\u00e4here Informationen zum Projekt erfahren Sie auf der Seite \\\\u00dcber Wikipedia sowie im Enzyklop\\\\u00e4die-Artikel zu Wikipedia. Die Bezeichnung \\\\u201efrei\\\\u201c bedeutet dabei, dass Wikipedia-Inhalte frei lizenziert sind. Demnach ist die private oder kommerzielle Verwendung und Weitergabe der in Wikipedia enthaltenen Texte \\\\u2013 sofern die Bestimmungen der GNU-Lizenz f\\\\u00fcr freie Dokumentation oder der Creative-Commons-Lizenz by-sa-3.0 eingehalten werden \\\\u2013 gestattet. F\\\\u00fcr hochgeladene Mediendateien, insbesondere Bilder, k\\\\u00f6nnen unterschiedliche freie Lizenzen gelten, die auf den Dateibeschreibungsseiten angegeben sind. Diese sind gesondert zu beachten, siehe auch Wikipedia:Lizenzbestimmungen.Bitte beachten Sie die ausf\\\\u00fchrlichen Nutzungsbedingungen der Wikimedia Foundation.Beachten Sie bitte die verbindliche Datenschutzrichtlinie der Wikimedia Foundation und die weiteren Hinweise zum Datenschutz.Die Wikipedia-Autoren sind aufgerufen, alle Inhalte, die sie zu dieser Website beisteuern, nach bestem Wissen zu erstellen. Die Inhalte der Wikipedia entstehen gemeinschaftlich, offen und ohne direkte redaktionelle Begleitung und Kontrolle. Praktisch jeder Inhalt kann jederzeit durch jedermann ver\\\\u00e4ndert werden. Insbesondere k\\\\u00f6nnen Werke, die unter dem Verdacht stehen, Urheber-, Verwertungs-, Pers\\\\u00f6nlichkeits- oder sonstige Rechte zu verletzen, unmittelbar ohne R\\\\u00fccksprache jederzeit korrigiert bzw. gel\\\\u00f6scht werden. Jeder Benutzer ist f\\\\u00fcr die von ihm erstellten Beitr\\\\u00e4ge selbst in vollem Umfang verantwortlich. Einen Ausschluss einzelner Benutzer beh\\\\u00e4lt sich die Anbieterin ausdr\\\\u00fccklich vor.Auf Grund der offenen Struktur der Wikipedia und der h\\\\u00e4ufigen \\\\u00c4nderungen ihrer Inhalte ist es m\\\\u00f6glich, dass Sie hier auf unrichtige, unvollst\\\\u00e4ndige, veraltete, widerspr\\\\u00fcchliche, in falschem Zusammenhang stehende oder verk\\\\u00fcrzte Angaben treffen. Die enzyklop\\\\u00e4dischen Artikel der Wikipedia dienen des Weiteren der allgemeinen Bildung und Weiterbildung, nicht der Beratung im Falle individueller Anliegen. Es kann deshalb keine Verantwortung f\\\\u00fcr Sch\\\\u00e4den \\\\u00fcbernommen werden, die durch das Vertrauen auf die Inhalte dieser Website oder deren Gebrauch entstehen. Bitte beachten Sie auch die gesonderten Hinweise zu Rechtsthemen und zu Gesundheitsthemen.Die als Administratoren oder \\\\u201eSysop\\\\u201c bezeichneten Benutzer der Wikipedia sowie die Benutzer mit sonstigen erweiterten Rechten sind keine offiziellen Vertreter der Website-Anbieterin, sondern lediglich Benutzer, denen weitergehende technische M\\\\u00f6glichkeiten einger\\\\u00e4umt wurden. Diese Personen helfen in der Regel gern und schnell bei eventuellen Fragen oder Beanstandungen, sind jedoch nicht pers\\\\u00f6nlich f\\\\u00fcr diese Website \\\\u2013 insbesondere nicht im Hinblick auf Richtigkeit, Aktualit\\\\u00e4t und Vollst\\\\u00e4ndigkeit der zur Verf\\\\u00fcgung gestellten Informationen \\\\u2013 verantwortlich.Die Website-Anbieterin weist auf allen Eingabeseiten darauf hin, kein Material zu verwenden, das Urheberrechten Dritter unterliegt. Bei der sehr gro\\\\u00dfen Zahl der in deutscher Sprache vorliegenden elektronischen und vor allem schriftlichen Publikationen kann aber nicht ausgeschlossen werden, dass von Benutzern dennoch unbefugt Material eingebracht wird, das bestehende Schutzrechte verletzt und das nicht sofort als solches erkannt wird.Wenn der Anbieterin eine entsprechende Urheberrechtsverletzung angezeigt wird, wird das betreffende Material umgehend entfernt. Offizieller Ansprechpartner f\\\\u00fcr solche F\\\\u00e4lle sind die nach US-amerikanischem Recht bestimmten Designated Agents von Wikimedia.Alternativ k\\\\u00f6nnen Sie sich auf informeller Basis auch an eine Gruppe von aktiven deutschsprachigen Benutzern wenden, die Sie unter info-de@wikimedia.org erreichen. Die E-Mail sollte den betroffenen Inhalt in Wikipedia genau bezeichnen (bitte immer URL und ggf. konkreten Abschnitt angeben) sowie die Publikation oder Website nennen, aus der das Material unberechtigt \\\\u00fcbernommen wurde.\", \"name\": \"Wikipedia:Impressum\"},\\n', 0)\n"
     ]
    }
   ],
   "source": [
    "results =list(zip(corpus, labels))\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing with HashingVectorizer\n",
    "\n",
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n",
    "\n",
    "A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n",
    "\n",
    "The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.\n",
    "\n",
    "The example below demonstrates the HashingVectorizer for encoding a single document.\n",
    "\n",
    "An arbitrary fixed-length vector size of 20 was chosen. This corresponds to the range of the hash function, where small values (like 20) may result in hash collisions. Remembering back to compsci classes, I believe there are heuristics that you can use to pick the hash length and probability of collision based on estimated vocabulary size.\n",
    "\n",
    "Note that this vectorizer does not require a call to fit on the training data documents. Instead, after instantiation, it can be used directly to start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 20)\n",
      "[[ 0.         -0.4472136   0.          0.         -0.4472136   0.\n",
      "   0.          0.          0.          0.          0.         -0.4472136\n",
      "   0.          0.          0.         -0.4472136   0.          0.4472136\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -0.5        -0.5\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.5         0.         -0.5         0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.57735027  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.57735027\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.57735027  0.        ]\n",
      " [ 0.          0.          0.          0.          0.         -0.5\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.5\n",
      "  -0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# create the transform\n",
    "hash_vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "hash_vector = hash_vectorizer.transform(dataset)\n",
    "# summarize encoded vector\n",
    "print(hash_vector.shape)\n",
    "print(hash_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_vectorizer.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'Fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana',\n",
       " 'Cat',\n",
       " 'sat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'mat',\n",
       " 'The',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'white.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list = [item for sublist in dataset for item in sublist.split()]\n",
    "flat_list\n",
    "# for sublist in dataset:\n",
    "#     for item in sublist.split():\n",
    "#         flat_list.append(item)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
