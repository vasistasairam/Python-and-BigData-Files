{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "## We shall use a sample text to create our preprocessing functions\n",
    "\n",
    "### Before that, lets perform some string manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A string is defined within quotes (single or double)\n",
    "#join() method returns a string in which string elements of sequence (B) have been joined by 'A' Seperator.\n",
    "A = 'Insofe'\n",
    "B = 'Batch45'\n",
    "C = 'X123'\n",
    "D = '123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BInsofeaInsofetInsofecInsofehInsofe4Insofe5\n",
      "B a t c h 4 5\n",
      "InsofeBatch45\n"
     ]
    }
   ],
   "source": [
    "print(A.join(B))\n",
    "print(\" \".join(B))\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insofe\n",
      "INSOFE\n",
      "-1\n",
      "0\n",
      "BInsofeaInsofetInsofecInsofehInsofe4Insofe5\n",
      "B a t c h 4 5\n",
      "InsofeBatch45\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Lets perform some string manipulations on the string\n",
    "print (A.lower())\n",
    "print(A.upper())\n",
    "print(A.find('a'))\n",
    "print(A.count('a'))\n",
    "print(A.join(B))\n",
    "print(' '.join(B))\n",
    "print(A+B)\n",
    "print(A.isdigit())\n",
    "print(D.isdigit())\n",
    "print(C.isdigit())\n",
    "print(C.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?A.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsofe\n",
      "Insofe\n",
      "Insofe\n",
      "Isf\n",
      "efosnI\n",
      "I\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-46af7b3845be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Seq[Start:End:Step]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'shr'\u001b[0m     \u001b[0;31m#Strings are immutable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "print(A.strip('I')) #Python String strip() method returns a copy of the string with both leading and trailing characters removed\n",
    "print(A)\n",
    "print(A[::1])  #Seq[Start:End:Step]\n",
    "print(A[::2])  #Seq[Start:End:Step]\n",
    "print(A[::-1]) #Seq[Start:End:Step]\n",
    "print(A[0])\n",
    "A[0]='shr'     #Strings are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''\n",
    "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'Leatherhead,', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes.', 'It', 'was', 'a', 'perfect', 'day,', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots,', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap,', 'his', 'arms', 'folded,', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes,', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast,', 'buried', 'in', 'the', 'deepest', 'thought.', 'Suddenly,', 'however,', 'he', 'started,', 'tapped', 'me', 'on', 'the', 'shoulder,', 'and', 'pointed', 'over', 'the', 'meadows.']\n"
     ]
    }
   ],
   "source": [
    "##Some string manipulations- Splitting the paragraph by spaces\n",
    "Str_list = string.split() # split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "print(Str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nAt Waterloo we were fortunate in catching a don't train for Leatherhead\", ' where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day', ' with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots', ' and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap', ' his arms folded', ' his hat pulled down over his eyes', ' and his chin sunk upon his breast', ' buried in the deepest thought. \\nSuddenly', ' however', ' he started', ' tapped me on the shoulder', ' and pointed over the meadows.\\n']\n"
     ]
    }
   ],
   "source": [
    "## Splitting the paragraph by a punctuation marks\n",
    "Str_list = string.split(\",\") #split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "print(Str_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if we want to read a text file?\n",
    "#### We first check the working directory using the \"os\" module and the \".getcwd()\" method in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os # The OS module in Python provides a way of using operating system dependent functionality. \n",
    "# Return information identifying the current operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And then set it to required path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/isenb047/Downloads/Text Preprocessing and SVD/20181124_Batch48_CSE7124c_TextMiningIntroduction'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we could even verify what files are there in the path as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20181124_Batch48_CSE7124c_TextPreprocessing.ipynb', '20181124_Batch48_CSE7124c_TextMiningIntroduction_ActivitySolution.ipynb', '20181124_Batch48_CSE7124c_TFIDF.ipynb', '.ipynb_checkpoints', '20181124_Batch48_CSE7124c_TfIdfManualCalculation.xlsx', 'english_german_articles.txt', 'sherlock.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have our required text file(s)/folder(s) in the working directory we use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sherlock.txt', 'r') as x:\n",
    "    string1 = x.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'r' in the code stands for read operation. One would use 'w' to write to a file and 'a' to append to an existing file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that our sample text is ready, let us perform the following steps\n",
    "1. Sentence Tokenizing\n",
    "2. Word Tokenizing\n",
    "3. Stop Word Removal\n",
    "4. Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nAt Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\", 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.', 'The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.', 'To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.', 'My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.', 'Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "\n",
      "\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "\n",
      "\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "\n",
      "\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "\n",
      "\n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "\n",
      "\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    print('\\n')\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above list notice how \"don't\" has been tokenized to 'do' and \"n't\" in word tokens in the above case. This wouldn't have been the case if we did a ```string.split()``` based on spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>[ Character set. Match any character in the set/\n",
    "<br>\t\\w Word. Matches any word character (alphanumeric & underscore).\n",
    "<br>\t' Character. Matches a ' Character\n",
    "<br>] \n",
    "<br>\n",
    "<br>+ Quantifier. Match 1 or more of the preceding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', 'It', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'Suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "####Using regular expressions\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens=tokenizer.tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case lowering\n",
    "```python\n",
    "lower_tokens = [] #empty list\n",
    "for token in tokens:\n",
    "    lower_tokens.append(token.lower())\n",
    "```\n",
    "#### But here's a nice use of list comprehensions. Instead of initiating an empty list, writing a for loop and keep appending lower case words, we can save much space by doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'the', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'to', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'my', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "print(lower_tokens,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for token in lower_tokens if token not in stop]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'comput', 'comput', 'comput', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using porter stemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','computer','computing','compute','better']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'comput', 'comput', 'comput', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using porter stemmer\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','computer','computing','compute','better']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in plurals]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing the plurals example text\n",
    "tokensLmtz = [lmtzr.lemmatize(token) for token in plurals]\n",
    "print(tokensLmtz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?lmtzr.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Removal\n",
    "Rmove punctuation, as it doesn’t add any extra information while treating text data. \n",
    "Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "singles = re.findall(r'\\w+', raw)\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine all the above functions into a single function\n",
    "#### Take this up as an exercise and try to write your own function which takes in a string and does the following (in the same order)\n",
    "1. Sentenence Tokenize\n",
    "2. Word tokenize on each sentence\n",
    "3. Lower case\n",
    "4. Stop word removal\n",
    "5. Lemmatizing each word\n",
    "\n",
    "If the input string is \n",
    "```python\n",
    "'The quick brown fox. Jumped over the lazy dog.'\n",
    "```\n",
    "The output should be as follows\n",
    "```python\n",
    "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n",
    "```\n",
    "Write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(input_string):\n",
    "    ################################\n",
    "    ## Replace pass with your own ##\n",
    "    ## logic                      ## \n",
    "    ################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "```python\n",
    "    # Applying the necessary text preprocessing steps on the data\n",
    "    def process_text(text):\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        sentence_tokens = [nltk.tokenize.word_tokenize(sentence) for sentence in sentences]\n",
    "#         tokens = []\n",
    "#         for sentence in sentence_tokens:\n",
    "#             sent = []\n",
    "#             for word in sentence:\n",
    "#                 if word.lower() not in stop:\n",
    "#                     sent.append(word.lower())\n",
    "#             tokens.append(sent)\n",
    "        ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "        tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "        tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "        return tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "{'d', 'a', 'c', 'b'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "A = ['a', 'b', 'c', 'd', 'a', 'a']\n",
    "print(len(A))\n",
    "print(set(A))\n",
    "print(len(set(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'The quick brown fox. Jumped over the lazy dog.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Solution\n",
    "\n",
    "    # Applying the necessary text preprocessing steps on the data\n",
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [nltk.tokenize.word_tokenize(sentence) \n",
    "                       for sentence in sentences]\n",
    "#         tokens = []\n",
    "#         for sentence in sentence_tokens:\n",
    "#             sent = []\n",
    "#             for word in sentence:\n",
    "#                 if word.lower() not in stop:\n",
    "#                     sent.append(word.lower())\n",
    "#             tokens.append(sent)\n",
    "        ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    tokens = [[word.lower() for word in sent if word not in stop] \n",
    "              for sent in sentence_tokens]\n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox. Jumped over the lazy dog.\n",
      "[['the', 'quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "string = 'The quick brown fox. Jumped over the lazy dog.'\n",
    "processed_tokens = process_text(string)\n",
    "print(string)\n",
    "print(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of the word \"upon\" during initialization is 0\n"
     ]
    }
   ],
   "source": [
    "word_count = nltk.FreqDist() #WE INITIALIZE AN EMPTY FREQUENCY COUNTER\n",
    "print('Frequency of the word \"upon\" during initialization is {}'.\n",
    "      format(word_count['upon']))\n",
    "#Which means the default frequency is set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waterloo',\n",
       " 'fortunate',\n",
       " 'catching',\n",
       " \"don't\",\n",
       " 'train',\n",
       " 'leatherhead',\n",
       " 'hired',\n",
       " 'trap',\n",
       " 'station',\n",
       " 'inn',\n",
       " 'drove',\n",
       " 'four',\n",
       " 'five',\n",
       " 'miles',\n",
       " 'lovely',\n",
       " 'surrey',\n",
       " 'lanes',\n",
       " 'perfect',\n",
       " 'day',\n",
       " 'bright',\n",
       " 'sun',\n",
       " 'fleecy',\n",
       " 'clouds',\n",
       " 'heavens',\n",
       " 'trees',\n",
       " 'wayside',\n",
       " 'hedges',\n",
       " 'throwing',\n",
       " 'first',\n",
       " 'green',\n",
       " 'shoots',\n",
       " 'air',\n",
       " 'full',\n",
       " 'pleasant',\n",
       " 'smell',\n",
       " 'moist',\n",
       " 'earth',\n",
       " 'least',\n",
       " 'strange',\n",
       " 'contrast',\n",
       " 'sweet',\n",
       " 'promise',\n",
       " 'spring',\n",
       " 'sinister',\n",
       " 'quest',\n",
       " 'upon',\n",
       " 'engaged',\n",
       " 'companion',\n",
       " 'sat',\n",
       " 'front',\n",
       " 'trap',\n",
       " 'arms',\n",
       " 'folded',\n",
       " 'hat',\n",
       " 'pulled',\n",
       " 'eyes',\n",
       " 'chin',\n",
       " 'sunk',\n",
       " 'upon',\n",
       " 'breast',\n",
       " 'buried',\n",
       " 'deepest',\n",
       " 'thought',\n",
       " 'suddenly',\n",
       " 'however',\n",
       " 'started',\n",
       " 'tapped',\n",
       " 'shoulder',\n",
       " 'pointed',\n",
       " 'meadows']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of the word \"upon\" after update is 3\n"
     ]
    }
   ],
   "source": [
    "#We update the frequency for each word as follows.\n",
    "for token in tokens:\n",
    "    word_count[token] += 1\n",
    "print('Frequency of the word \"upon\" after update is {}'.format(word_count['upon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('upon', 3), ('trap', 2), ('once', 1), ('waterloo', 1), ('fortunate', 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trap', 2), ('upon', 2), ('waterloo', 1), ('fortunate', 1), ('catching', 1), ('train', 1), ('leatherhead', 1), ('hired', 1), ('station', 1), ('inn', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(word_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping wordcount into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('word_freqs.json', 'w') as f:\n",
    "    json.dump(word_count, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "<br>N-grams are the combination of multiple words used together. \n",
    "<br>Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "<br>Unigrams do not usually contain as much information as compared to bigrams and trigrams. \n",
    "<br>The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n",
    "<br>The longer the n-gram (the higher the n), the more context you have to work with. \n",
    "<br>Optimum length really depends on the application \n",
    "– if your n-grams are too short, you may fail to capture important differences. \n",
    "<br>On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('waterloo', 'fortunate')\n",
      "('fortunate', 'catching')\n",
      "('catching', \"don't\")\n",
      "(\"don't\", 'train')\n",
      "('train', 'leatherhead')\n",
      "('leatherhead', 'hired')\n",
      "('hired', 'trap')\n",
      "('trap', 'station')\n",
      "('station', 'inn')\n",
      "('inn', 'drove')\n",
      "('drove', 'four')\n",
      "('four', 'five')\n",
      "('five', 'miles')\n",
      "('miles', 'lovely')\n",
      "('lovely', 'surrey')\n",
      "('surrey', 'lanes')\n",
      "('lanes', 'perfect')\n",
      "('perfect', 'day')\n",
      "('day', 'bright')\n",
      "('bright', 'sun')\n",
      "('sun', 'fleecy')\n",
      "('fleecy', 'clouds')\n",
      "('clouds', 'heavens')\n",
      "('heavens', 'trees')\n",
      "('trees', 'wayside')\n",
      "('wayside', 'hedges')\n",
      "('hedges', 'throwing')\n",
      "('throwing', 'first')\n",
      "('first', 'green')\n",
      "('green', 'shoots')\n",
      "('shoots', 'air')\n",
      "('air', 'full')\n",
      "('full', 'pleasant')\n",
      "('pleasant', 'smell')\n",
      "('smell', 'moist')\n",
      "('moist', 'earth')\n",
      "('earth', 'least')\n",
      "('least', 'strange')\n",
      "('strange', 'contrast')\n",
      "('contrast', 'sweet')\n",
      "('sweet', 'promise')\n",
      "('promise', 'spring')\n",
      "('spring', 'sinister')\n",
      "('sinister', 'quest')\n",
      "('quest', 'upon')\n",
      "('upon', 'engaged')\n",
      "('engaged', 'companion')\n",
      "('companion', 'sat')\n",
      "('sat', 'front')\n",
      "('front', 'trap')\n",
      "('trap', 'arms')\n",
      "('arms', 'folded')\n",
      "('folded', 'hat')\n",
      "('hat', 'pulled')\n",
      "('pulled', 'eyes')\n",
      "('eyes', 'chin')\n",
      "('chin', 'sunk')\n",
      "('sunk', 'upon')\n",
      "('upon', 'breast')\n",
      "('breast', 'buried')\n",
      "('buried', 'deepest')\n",
      "('deepest', 'thought')\n",
      "('thought', 'suddenly')\n",
      "('suddenly', 'however')\n",
      "('however', 'started')\n",
      "('started', 'tapped')\n",
      "('tapped', 'shoulder')\n",
      "('shoulder', 'pointed')\n",
      "('pointed', 'meadows')\n"
     ]
    }
   ],
   "source": [
    "for each_bigram in nltk.ngrams(tokens, 2):\n",
    "    print (each_bigram,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('waterloo', 'fortunate', 'catching')\n",
      "('fortunate', 'catching', \"don't\")\n",
      "('catching', \"don't\", 'train')\n",
      "(\"don't\", 'train', 'leatherhead')\n",
      "('train', 'leatherhead', 'hired')\n",
      "('leatherhead', 'hired', 'trap')\n",
      "('hired', 'trap', 'station')\n",
      "('trap', 'station', 'inn')\n",
      "('station', 'inn', 'drove')\n",
      "('inn', 'drove', 'four')\n",
      "('drove', 'four', 'five')\n",
      "('four', 'five', 'miles')\n",
      "('five', 'miles', 'lovely')\n",
      "('miles', 'lovely', 'surrey')\n",
      "('lovely', 'surrey', 'lanes')\n",
      "('surrey', 'lanes', 'perfect')\n",
      "('lanes', 'perfect', 'day')\n",
      "('perfect', 'day', 'bright')\n",
      "('day', 'bright', 'sun')\n",
      "('bright', 'sun', 'fleecy')\n",
      "('sun', 'fleecy', 'clouds')\n",
      "('fleecy', 'clouds', 'heavens')\n",
      "('clouds', 'heavens', 'trees')\n",
      "('heavens', 'trees', 'wayside')\n",
      "('trees', 'wayside', 'hedges')\n",
      "('wayside', 'hedges', 'throwing')\n",
      "('hedges', 'throwing', 'first')\n",
      "('throwing', 'first', 'green')\n",
      "('first', 'green', 'shoots')\n",
      "('green', 'shoots', 'air')\n",
      "('shoots', 'air', 'full')\n",
      "('air', 'full', 'pleasant')\n",
      "('full', 'pleasant', 'smell')\n",
      "('pleasant', 'smell', 'moist')\n",
      "('smell', 'moist', 'earth')\n",
      "('moist', 'earth', 'least')\n",
      "('earth', 'least', 'strange')\n",
      "('least', 'strange', 'contrast')\n",
      "('strange', 'contrast', 'sweet')\n",
      "('contrast', 'sweet', 'promise')\n",
      "('sweet', 'promise', 'spring')\n",
      "('promise', 'spring', 'sinister')\n",
      "('spring', 'sinister', 'quest')\n",
      "('sinister', 'quest', 'upon')\n",
      "('quest', 'upon', 'engaged')\n",
      "('upon', 'engaged', 'companion')\n",
      "('engaged', 'companion', 'sat')\n",
      "('companion', 'sat', 'front')\n",
      "('sat', 'front', 'trap')\n",
      "('front', 'trap', 'arms')\n",
      "('trap', 'arms', 'folded')\n",
      "('arms', 'folded', 'hat')\n",
      "('folded', 'hat', 'pulled')\n",
      "('hat', 'pulled', 'eyes')\n",
      "('pulled', 'eyes', 'chin')\n",
      "('eyes', 'chin', 'sunk')\n",
      "('chin', 'sunk', 'upon')\n",
      "('sunk', 'upon', 'breast')\n",
      "('upon', 'breast', 'buried')\n",
      "('breast', 'buried', 'deepest')\n",
      "('buried', 'deepest', 'thought')\n",
      "('deepest', 'thought', 'suddenly')\n",
      "('thought', 'suddenly', 'however')\n",
      "('suddenly', 'however', 'started')\n",
      "('however', 'started', 'tapped')\n",
      "('started', 'tapped', 'shoulder')\n",
      "('tapped', 'shoulder', 'pointed')\n",
      "('shoulder', 'pointed', 'meadows')\n"
     ]
    }
   ],
   "source": [
    "for each_bigram in nltk.ngrams(tokens, 3):\n",
    "    print (each_bigram,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "* How many words are there in the text? \n",
    "* How many sentences are there in the text?\n",
    "* How many unique words are there in the text?\n",
    "* What is the average number of characters of a word in the text?\n",
    "* Remove the stop words and store the tokens in a list.\n",
    "* What are the 10 most common words in the processed text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Solutions to assignment 2\n",
    "string='''At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'the', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'to', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'my', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(string)\n",
    "\n",
    "tokens = [token.lower() for token in tokens]\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "6\n",
      "99\n",
      "4.267123287671233\n",
      "69\n",
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'companion', 'sat', 'front', 'trap', 'arms', 'folded', 'hat', 'pulled', 'eyes', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadows']\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#1.\n",
    "print(len(tokens))\n",
    "\n",
    "#2. Sentences\n",
    "sentences = sent_tokenize(string)\n",
    "print(len(sentences))\n",
    "\n",
    "#3. unique words\n",
    "print(len(set(tokens)))\n",
    "\n",
    "#4. Average word length\n",
    "print (np.mean([len(token) for token in tokens]))\n",
    "\n",
    "#5. remove stopwords\n",
    "word_actual = [word for word in tokens if word not in stop]\n",
    "print(len(word_actual))\n",
    "print(word_actual)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = nltk.FreqDist()\n",
    "for token in tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_keys'>\n",
      "dict_keys(['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better'])\n"
     ]
    }
   ],
   "source": [
    "print(type(word_count.keys()))\n",
    "print(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caress': 1, 'fly': 1, 'dy': 1, 'mule': 1, 'denied': 1, 'computer': 1, 'computing': 1, 'compute': 1, 'better': 1}\n"
     ]
    }
   ],
   "source": [
    "print(dict(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better']\n"
     ]
    }
   ],
   "source": [
    "A = list(word_count.keys())\n",
    "print(type(A[0]))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n",
      "dict_keys(['Sachin', 'Ramesh', 'Tendulkar', ';', 'born', '24', 'April', '1973', 'is', 'a', 'former', 'Indian', 'international', 'cricketer', 'and', 'captain', 'of', 'the', 'national', 'team', ',', 'regarded', 'as', 'one', 'greatest', 'batsmen', 'all', 'time', '.', '[', '4', ']', 'He', 'highest', 'run', 'scorer', 'in', 'International', 'cricket', 'Often', 'referred', 'to', \"'God\", 'Cricket', \"'\", 'by', 'followers', '5', 'took', 'up', 'at', 'age', 'eleven', 'made', 'his', 'Test', 'debut', 'on', '15', 'November', '1989', 'against', 'Pakistan', 'Karachi', 'sixteen', 'went', 'represent', 'Mumbai', 'domestically', 'India', 'internationally', 'for', 'close', 'twenty-four', 'years', 'only', 'player', 'have', 'scored', 'hundred', 'centuries', 'first', 'batsman', 'score', 'double', 'century', 'ODI', 'holder', 'record', 'most', 'number', 'runs', 'both', 'complete', 'more', 'than', '30,000', '6', 'In', '2002', 'halfway', 'through', 'career', 'Wisden', 'Cricketers', 'Almanack', 'ranked', 'him', 'second', 'behind', 'Don', 'Bradman', 'Viv', 'Richards', '7', 'Later', 'was', 'part', 'that', 'won', '2011', 'World', 'Cup', 'win', 'six', 'appearances', '8', 'had', 'previously', 'been', 'named', '``', 'Player', 'Tournament', \"''\", '2003', 'edition', 'tournament', 'held', 'South', 'Africa', '2013', 'he', 'included', 'an', 'all-time', 'XI', 'mark', '150th', 'anniversary', '9', '10', '11', 'received', 'Arjuna', 'Award', '1994', 'outstanding', 'sporting', 'achievement', 'Rajiv', 'Gandhi', 'Khel', 'Ratna', 'award', '1997', \"'s\", 'honour', 'Padma', 'Shri', 'Vibhushan', 'awards', '1999', '2008', 'respectively', 'fourth', 'civilian', '12', 'After', 'few', 'hours', 'final', 'match', '16', 'Prime', 'Minister', 'Office', 'announced', 'decision', 'Bharat', '13', '14', 'youngest', 'recipient', 'date', 'ever', 'sportsperson', 'receive', 'also', '2010', 'Sir', 'Garfield', 'Sobers', 'Trophy', 'year', 'ICC', '17', '2012', 'nominated', 'Rajya', 'Sabha', 'upper', 'house', 'Parliament', '18', 'person', 'without', 'aviation', 'background', 'be', 'awarded', 'honorary', 'rank', 'group', 'Air', 'Force', '19', 'Honorary', 'Member', 'Order', 'Australia', '20', '21', 'Time', 'magazine', 'its', 'annual', '100', 'list', 'Most', 'Influential', 'People', '22', 'December', 'retirement', 'from', 'ODIs', '23', 'retired', 'Twenty20', 'October', 'subsequently', 'forms', 'after', 'playing', '200th', 'West', 'Indies', 'Wankhede', 'Stadium', '25', 'played', '664', 'matches', 'total', 'scoring', '34,357'])\n"
     ]
    }
   ],
   "source": [
    "##Generation of wordcloud\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "text = '''Sachin Ramesh Tendulkar; born 24 April 1973 is a former Indian international cricketer and a former captain of the Indian national team, regarded as one of the greatest batsmen of all time.[4] He is the highest run scorer of all time in International cricket. Often referred to as the 'God of Cricket' by Indian cricket followers,[5] Tendulkar took up cricket at the age of eleven, made his Test debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for close to twenty-four years. He is the only player to have scored one hundred international centuries, the first batsman to score a double century in a ODI, the holder of the record for the most number of runs in both Test and ODI, and the only player to complete more than 30,000 runs in international cricket.[6]\n",
    "\n",
    "In 2002, halfway through his career, Wisden Cricketers' Almanack ranked him the second greatest Test batsman of all time, behind Don Bradman, and the second greatest ODI batsman of all time, behind Viv Richards.[7] Later in his career, Tendulkar was a part of the Indian team that won the 2011 World Cup, his first win in six World Cup appearances for India.[8] He had previously been named \"Player of the Tournament\" at the 2003 edition of the tournament, held in South Africa. In 2013, he was the only Indian cricketer included in an all-time Test World XI named to mark the 150th anniversary of Wisden Cricketers' Almanack.[9][10][11]\n",
    "\n",
    "Tendulkar received the Arjuna Award in 1994 for his outstanding sporting achievement, the Rajiv Gandhi Khel Ratna award in 1997, India's highest sporting honour, and the Padma Shri and Padma Vibhushan awards in 1999 and 2008, respectively, India's fourth and second highest civilian awards.[12] After a few hours of his final match on 16 November 2013, the Prime Minister's Office announced the decision to award him the Bharat Ratna, India's highest civilian award.[13][14] He is the youngest recipient to date and the first ever sportsperson to receive the award.[15][16] He also won the 2010 Sir Garfield Sobers Trophy for cricketer of the year at the ICC awards.[17] In 2012, Tendulkar was nominated to the Rajya Sabha, the upper house of the Parliament of India.[18] He was also the first sportsperson and the first person without an aviation background to be awarded the honorary rank of group captain by the Indian Air Force.[19] In 2012, he was named an Honorary Member of the Order of Australia.[20][21]\n",
    "\n",
    "In 2010, Time magazine included Sachin in its annual Time 100 list as one of the \"Most Influential People in the World\".[22] In December 2012, Tendulkar announced his retirement from ODIs.[23] He retired from Twenty20 cricket in October 2013[24] and subsequently retired from all forms of cricket on 16 November 2013 after playing his 200th Test match, against the West Indies in Mumbai's Wankhede Stadium.[25] Tendulkar played 664 international cricket matches in total, scoring 34,357 runs.[6]'''\n",
    "\n",
    "A = word_tokenize(text)\n",
    "B = nltk.FreqDist(A)\n",
    "print(type(B))\n",
    "#print(B.items())\n",
    "print(B.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin : 2\n",
      "Ramesh : 1\n",
      "Tendulkar : 7\n",
      "; : 1\n",
      "born : 1\n",
      "24 : 2\n",
      "April : 1\n",
      "1973 : 1\n",
      "is : 4\n",
      "a : 6\n",
      "former : 2\n",
      "Indian : 6\n",
      "international : 4\n",
      "cricketer : 3\n",
      "and : 13\n",
      "captain : 2\n",
      "of : 24\n",
      "the : 44\n",
      "national : 1\n",
      "team : 2\n",
      ", : 30\n",
      "regarded : 1\n",
      "as : 3\n",
      "one : 3\n",
      "greatest : 3\n",
      "batsmen : 1\n",
      "all : 5\n",
      "time : 4\n",
      ". : 19\n",
      "[ : 23\n",
      "4 : 1\n",
      "] : 23\n",
      "He : 7\n",
      "highest : 4\n",
      "run : 1\n",
      "scorer : 1\n",
      "in : 17\n",
      "International : 1\n",
      "cricket : 7\n",
      "Often : 1\n",
      "referred : 1\n",
      "to : 12\n",
      "'God : 1\n",
      "Cricket : 1\n",
      "' : 3\n",
      "by : 2\n",
      "followers : 1\n",
      "5 : 1\n",
      "took : 1\n",
      "up : 1\n",
      "at : 4\n",
      "age : 2\n",
      "eleven : 1\n",
      "made : 1\n",
      "his : 8\n",
      "Test : 5\n",
      "debut : 1\n",
      "on : 4\n",
      "15 : 2\n",
      "November : 3\n",
      "1989 : 1\n",
      "against : 2\n",
      "Pakistan : 1\n",
      "Karachi : 1\n",
      "sixteen : 1\n",
      "went : 1\n",
      "represent : 1\n",
      "Mumbai : 2\n",
      "domestically : 1\n",
      "India : 6\n",
      "internationally : 1\n",
      "for : 5\n",
      "close : 1\n",
      "twenty-four : 1\n",
      "years : 1\n",
      "only : 3\n",
      "player : 2\n",
      "have : 1\n",
      "scored : 1\n",
      "hundred : 1\n",
      "centuries : 1\n",
      "first : 5\n",
      "batsman : 3\n",
      "score : 1\n",
      "double : 1\n",
      "century : 1\n",
      "ODI : 3\n",
      "holder : 1\n",
      "record : 1\n",
      "most : 1\n",
      "number : 1\n",
      "runs : 3\n",
      "both : 1\n",
      "complete : 1\n",
      "more : 1\n",
      "than : 1\n",
      "30,000 : 1\n",
      "6 : 2\n",
      "In : 6\n",
      "2002 : 1\n",
      "halfway : 1\n",
      "through : 1\n",
      "career : 2\n",
      "Wisden : 2\n",
      "Cricketers : 2\n",
      "Almanack : 2\n",
      "ranked : 1\n",
      "him : 2\n",
      "second : 3\n",
      "behind : 2\n",
      "Don : 1\n",
      "Bradman : 1\n",
      "Viv : 1\n",
      "Richards : 1\n",
      "7 : 1\n",
      "Later : 1\n",
      "was : 5\n",
      "part : 1\n",
      "that : 1\n",
      "won : 2\n",
      "2011 : 1\n",
      "World : 4\n",
      "Cup : 2\n",
      "win : 1\n",
      "six : 1\n",
      "appearances : 1\n",
      "8 : 1\n",
      "had : 1\n",
      "previously : 1\n",
      "been : 1\n",
      "named : 3\n",
      "`` : 2\n",
      "Player : 1\n",
      "Tournament : 1\n",
      "'' : 2\n",
      "2003 : 1\n",
      "edition : 1\n",
      "tournament : 1\n",
      "held : 1\n",
      "South : 1\n",
      "Africa : 1\n",
      "2013 : 4\n",
      "he : 2\n",
      "included : 2\n",
      "an : 3\n",
      "all-time : 1\n",
      "XI : 1\n",
      "mark : 1\n",
      "150th : 1\n",
      "anniversary : 1\n",
      "9 : 1\n",
      "10 : 1\n",
      "11 : 1\n",
      "received : 1\n",
      "Arjuna : 1\n",
      "Award : 1\n",
      "1994 : 1\n",
      "outstanding : 1\n",
      "sporting : 2\n",
      "achievement : 1\n",
      "Rajiv : 1\n",
      "Gandhi : 1\n",
      "Khel : 1\n",
      "Ratna : 2\n",
      "award : 4\n",
      "1997 : 1\n",
      "'s : 5\n",
      "honour : 1\n",
      "Padma : 2\n",
      "Shri : 1\n",
      "Vibhushan : 1\n",
      "awards : 3\n",
      "1999 : 1\n",
      "2008 : 1\n",
      "respectively : 1\n",
      "fourth : 1\n",
      "civilian : 2\n",
      "12 : 1\n",
      "After : 1\n",
      "few : 1\n",
      "hours : 1\n",
      "final : 1\n",
      "match : 2\n",
      "16 : 3\n",
      "Prime : 1\n",
      "Minister : 1\n",
      "Office : 1\n",
      "announced : 2\n",
      "decision : 1\n",
      "Bharat : 1\n",
      "13 : 1\n",
      "14 : 1\n",
      "youngest : 1\n",
      "recipient : 1\n",
      "date : 1\n",
      "ever : 1\n",
      "sportsperson : 2\n",
      "receive : 1\n",
      "also : 2\n",
      "2010 : 2\n",
      "Sir : 1\n",
      "Garfield : 1\n",
      "Sobers : 1\n",
      "Trophy : 1\n",
      "year : 1\n",
      "ICC : 1\n",
      "17 : 1\n",
      "2012 : 3\n",
      "nominated : 1\n",
      "Rajya : 1\n",
      "Sabha : 1\n",
      "upper : 1\n",
      "house : 1\n",
      "Parliament : 1\n",
      "18 : 1\n",
      "person : 1\n",
      "without : 1\n",
      "aviation : 1\n",
      "background : 1\n",
      "be : 1\n",
      "awarded : 1\n",
      "honorary : 1\n",
      "rank : 1\n",
      "group : 1\n",
      "Air : 1\n",
      "Force : 1\n",
      "19 : 1\n",
      "Honorary : 1\n",
      "Member : 1\n",
      "Order : 1\n",
      "Australia : 1\n",
      "20 : 1\n",
      "21 : 1\n",
      "Time : 2\n",
      "magazine : 1\n",
      "its : 1\n",
      "annual : 1\n",
      "100 : 1\n",
      "list : 1\n",
      "Most : 1\n",
      "Influential : 1\n",
      "People : 1\n",
      "22 : 1\n",
      "December : 1\n",
      "retirement : 1\n",
      "from : 3\n",
      "ODIs : 1\n",
      "23 : 1\n",
      "retired : 2\n",
      "Twenty20 : 1\n",
      "October : 1\n",
      "subsequently : 1\n",
      "forms : 1\n",
      "after : 1\n",
      "playing : 1\n",
      "200th : 1\n",
      "West : 1\n",
      "Indies : 1\n",
      "Wankhede : 1\n",
      "Stadium : 1\n",
      "25 : 1\n",
      "played : 1\n",
      "664 : 1\n",
      "matches : 1\n",
      "total : 1\n",
      "scoring : 1\n",
      "34,357 : 1\n"
     ]
    }
   ],
   "source": [
    "#print(B.keys())\n",
    "for i in B:\n",
    "    print (i +\" : \"+ str(B[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d428c3f01eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcloud\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "cloud=WordCloud().generate(text)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cloud)\n",
    "plt.show(cloud)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
